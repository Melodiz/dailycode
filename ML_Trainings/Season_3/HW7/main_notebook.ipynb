{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice: Basic Artificial Neural Networks\n",
    "Credits: this notebook belongs to [Practical DL](https://docs.google.com/forms/d/e/1FAIpQLScvrVtuwrHSlxWqHnLt1V-_7h2eON_mlRR6MUb3xEe5x9LuoA/viewform?usp=sf_link) course by Yandex School of Data Analysis.\n",
    "\n",
    "We will start working with neural networks on the practice session. Your homework will be to finish the implementation of the layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is simple, yet an actual implementation may take some time :). We are going to write an Artificial Neural Network (almost) from scratch. The software design was heavily inspired by [PyTorch](http://pytorch.org) which is the main framework of our course "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speaking about the homework (once again, it will be really similar to this seminar), it requires sending **multiple** files, please do not forget to include all the files when sending to TA. The list of files:\n",
    "- This notebook\n",
    "- modules.ipynb with all blocks implemented (except maybe `Conv2d` and `MaxPool2d` layers implementation which are part of 'advanced' version of this homework)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from time import time, sleep\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement everything in `modules.ipynb`. Read all the comments thoughtfully to ease the pain. Please try not to change the prototypes.\n",
    "\n",
    "Do not forget, that each module should return **AND** store `output` and `gradInput`.\n",
    "\n",
    "The typical assumption is that `module.backward` is always executed after `module.forward`,\n",
    "so `output` is stored, this would be useful for `SoftMax`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tech note\n",
    "Prefer using `np.multiply`, `np.add`, `np.divide`, `np.subtract` instead of `*`,`+`,`/`,`-` for better memory handling.\n",
    "\n",
    "Example: suppose you allocated a variable \n",
    "\n",
    "```\n",
    "a = np.zeros(...)\n",
    "```\n",
    "So, instead of\n",
    "```\n",
    "a = b + c  # will be reallocated, GC needed to free\n",
    "``` \n",
    "You can use: \n",
    "```\n",
    "np.add(b,c,out = a) # puts result in `a`\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (re-)load layers\n",
    "%run modules.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this example to debug your code, start with logistic regression and then test other layers. You do not need to change anything here. This code is provided for you to test the layers. Also it is easy to use this code in MNIST task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x143a7ba10>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate some data\n",
    "N = 500\n",
    "\n",
    "X1 = np.random.randn(N,2) + np.array([2,2])\n",
    "X2 = np.random.randn(N,2) + np.array([-2,-2])\n",
    "\n",
    "Y = np.concatenate([np.ones(N),np.zeros(N)])[:,None]\n",
    "Y = np.hstack([Y, 1-Y])\n",
    "\n",
    "X = np.vstack([X1,X2])\n",
    "plt.scatter(X[:,0],X[:,1], c = Y[:,0], edgecolors= 'none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a **logistic regression** for debugging. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear 2 -> 2\n",
      "LogSoftMax\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net = Sequential()\n",
    "net.add(Linear(2, 2))\n",
    "net.add(LogSoftMax())\n",
    "\n",
    "criterion = ClassNLLCriterion()\n",
    "\n",
    "print(net)\n",
    "\n",
    "# Test something like that then \n",
    "\n",
    "# net = Sequential()\n",
    "# net.add(Linear(2, 4))\n",
    "# net.add(ReLU())\n",
    "# net.add(Linear(4, 2))\n",
    "# net.add(LogSoftMax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with batch_size = 1000 to make sure every step lowers the loss, then try stochastic version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iptimizer params\n",
    "optimizer_config = {'learning_rate' : 1e-1, 'momentum': 0.9}\n",
    "optimizer_state = {}\n",
    "\n",
    "# Looping params\n",
    "n_epoch = 20\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch generator\n",
    "def get_batches(dataset, batch_size):\n",
    "    X, Y = dataset\n",
    "    n_samples = X.shape[0]\n",
    "        \n",
    "    # Shuffle at the start of epoch\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        \n",
    "        batch_idx = indices[start:end]\n",
    "    \n",
    "        yield X[batch_idx], Y[batch_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic training loop. Examine it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss: 0.004739\n"
     ]
    }
   ],
   "source": [
    "loss_history = []\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    for x_batch, y_batch in get_batches((X, Y), batch_size):\n",
    "        \n",
    "        net.zeroGradParameters()\n",
    "        \n",
    "        # Forward\n",
    "        predictions = net.forward(x_batch)\n",
    "        loss = criterion.forward(predictions, y_batch)\n",
    "    \n",
    "        # Backward\n",
    "        dp = criterion.backward(predictions, y_batch)\n",
    "        net.backward(x_batch, dp)\n",
    "        \n",
    "        # Update weights\n",
    "        sgd_momentum(net.getParameters(), \n",
    "                     net.getGradParameters(), \n",
    "                     optimizer_config,\n",
    "                     optimizer_state)      \n",
    "        \n",
    "        loss_history.append(loss)\n",
    "\n",
    "    # Visualize\n",
    "    display.clear_output(wait=True)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "        \n",
    "    plt.title(\"Training loss\")\n",
    "    plt.xlabel(\"#iteration\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.plot(loss_history, 'b')\n",
    "    plt.show()\n",
    "    \n",
    "    print('Current loss: %f' % loss)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using old good [MNIST](http://yann.lecun.com/exdb/mnist/) as our dataset. It can be downloaded with the following file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘mnist.py’ already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/girafe-ai/ml-course/23f_basic/homeworks/hw08_nn_from_scratch/mnist.py -nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "__doc__=\"\"\"taken from https://github.com/Lasagne/Lasagne/blob/master/examples/mnist.py\"\"\"\n",
    "\n",
    "def load_dataset(flatten=False):\n",
    "    # We first define a download function, supporting both Python 2 and 3.\n",
    "    if sys.version_info[0] == 2:\n",
    "        from urllib import urlretrieve\n",
    "    else:\n",
    "        from urllib.request import urlretrieve\n",
    "\n",
    "    def download(filename, source='https://storage.googleapis.com/cvdf-datasets/mnist/'):\n",
    "        print(\"Downloading %s\" % filename)\n",
    "        urlretrieve(source + filename, filename)\n",
    "\n",
    "    # We then define functions for loading MNIST images and labels.\n",
    "    # For convenience, they also download the requested files if needed.\n",
    "    import gzip\n",
    "\n",
    "    def load_mnist_images(filename):\n",
    "        if not os.path.exists(filename):\n",
    "            download(filename)\n",
    "        # Read the inputs in Yann LeCun's binary format.\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "        # The inputs are vectors now, we reshape them to monochrome 2D images,\n",
    "        # following the shape convention: (examples, channels, rows, columns)\n",
    "        data = data.reshape(-1, 1, 28, 28)\n",
    "        # The inputs come as bytes, we convert them to float32 in range [0,1].\n",
    "        # (Actually to range [0, 255/256], for compatibility to the version\n",
    "        # provided at http://deeplearning.net/data/mnist/mnist.pkl.gz.)\n",
    "        return (data / np.float32(256)).squeeze()\n",
    "\n",
    "    def load_mnist_labels(filename):\n",
    "        if not os.path.exists(filename):\n",
    "            download(filename)\n",
    "        # Read the labels in Yann LeCun's binary format.\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "        # The labels are vectors of integers now, that's exactly what we want.\n",
    "        return data\n",
    "\n",
    "    # We can now download and read the training and test set images and labels.\n",
    "    X_train = load_mnist_images('train-images-idx3-ubyte.gz')\n",
    "    y_train = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
    "    X_test = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
    "    y_test = load_mnist_labels('t10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "    # We reserve the last 10000 training examples for validation.\n",
    "    X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
    "    y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
    "\n",
    "    if flatten:\n",
    "        X_train = X_train.reshape([-1, 28**2])\n",
    "        X_val = X_val.reshape([-1, 28**2])\n",
    "        X_test = X_test.reshape([-1, 28**2])\n",
    "\n",
    "\n",
    "    # We just return all the arrays in order, as expected in main().\n",
    "    # (It doesn't matter how we do this as long as we can read them again.)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encode the labels first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original y_train shape: (50000,)\n",
      "One-hot encoded y_train shape: (50000, 10)\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(y, num_classes=10):\n",
    "    return np.eye(num_classes)[y]\n",
    "\n",
    "y_train_one_hot = one_hot_encode(y_train)\n",
    "y_val_one_hot = one_hot_encode(y_val)\n",
    "y_test_one_hot = one_hot_encode(y_test)\n",
    "\n",
    "print(\"Original y_train shape:\", y_train.shape)\n",
    "print(\"One-hot encoded y_train shape:\", y_train_one_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Compare** `ReLU`, `ELU`, `LeakyReLU`, `SoftPlus` activation functions. \n",
    "You would better pick the best optimizer params for each of them, but it is overkill for now. Use an architecture of your choice for the comparison.\n",
    "- **Try** inserting `BatchNormalization` (folowed by `ChannelwiseScaling`) between `Linear` module and activation functions.\n",
    "- Plot the losses both from activation functions comparison and `BatchNormalization` comparison on one plot. Please find a scale (log?) when the lines are distinguishable, do not forget about naming the axes, the plot should be goodlooking.\n",
    "- Plot the losses for two networks: one trained by momentum_sgd, another one trained by Adam. Which one performs better?\n",
    "- Hint: good logloss for MNIST should be around 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here. ################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your personal opinion on the activation functions, think about computation times too. Does `BatchNormalization` help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer goes here. ################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally**, use all your knowledge to build a super cool model on this dataset. Use **dropout** to prevent overfitting, play with **learning rate decay**. You can use **data augmentation** such as rotations, translations to boost your score. Use your knowledge and imagination to train a model. Don't forget to call `training()` and `evaluate()` methods to set desired behaviour of `BatchNormalization` and `Dropout` layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ReLU model...\n",
      "Epoch 1/5, Train Loss: 0.6878, Val Loss: 0.3166\n",
      "Epoch 2/5, Train Loss: 0.3150, Val Loss: 0.2613\n",
      "Epoch 3/5, Train Loss: 0.2653, Val Loss: 0.2280\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 86\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Train models\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining ReLU model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 86\u001b[0m relu_train_loss, relu_val_loss \u001b[38;5;241m=\u001b[39m train_model(\n\u001b[1;32m     87\u001b[0m     relu_model, criterion, optimizer_config, sgd_momentum, \n\u001b[1;32m     88\u001b[0m     X_train_flat, y_train_one_hot, X_val_flat, y_val_one_hot, n_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[1;32m     89\u001b[0m )\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining ELU model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     92\u001b[0m elu_train_loss, elu_val_loss \u001b[38;5;241m=\u001b[39m train_model(\n\u001b[1;32m     93\u001b[0m     elu_model, criterion, optimizer_config, sgd_momentum, \n\u001b[1;32m     94\u001b[0m     X_train_flat, y_train_one_hot, X_val_flat, y_val_one_hot, n_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[1;32m     95\u001b[0m )\n",
      "Cell \u001b[0;32mIn[65], line 30\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer_config, optimizer_func, X_train, y_train, X_val, y_val, n_epochs, batch_size)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[1;32m     29\u001b[0m dp \u001b[38;5;241m=\u001b[39m criterion\u001b[38;5;241m.\u001b[39mbackward(predictions, y_batch)\n\u001b[0;32m---> 30\u001b[0m model\u001b[38;5;241m.\u001b[39mbackward(x_batch, dp)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[1;32m     33\u001b[0m optimizer_func(model\u001b[38;5;241m.\u001b[39mgetParameters(), \n\u001b[1;32m     34\u001b[0m              model\u001b[38;5;241m.\u001b[39mgetGradParameters(), \n\u001b[1;32m     35\u001b[0m              optimizer_config,\n\u001b[1;32m     36\u001b[0m              optimizer_state)\n",
      "File \u001b[0;32m/var/folders/d3/zcslj2712p57dfxhz77sfh8r0000gn/T/ipykernel_47963/2520834772.py:74\u001b[0m, in \u001b[0;36mSequential.backward\u001b[0;34m(self, input, gradOutput)\u001b[0m\n\u001b[1;32m     71\u001b[0m current_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 74\u001b[0m     current_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules[i]\u001b[38;5;241m.\u001b[39mforward(current_input)\n\u001b[1;32m     75\u001b[0m     inputs\u001b[38;5;241m.\u001b[39mappend(current_input)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Now perform the backward pass\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/d3/zcslj2712p57dfxhz77sfh8r0000gn/T/ipykernel_47963/1342133365.py:24\u001b[0m, in \u001b[0;36mModule.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m     21\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m    Takes an input object, and computes the corresponding output of the module.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdateOutput(\u001b[38;5;28minput\u001b[39m)\n",
      "File \u001b[0;32m/var/folders/d3/zcslj2712p57dfxhz77sfh8r0000gn/T/ipykernel_47963/2032222597.py:22\u001b[0m, in \u001b[0;36mLinear.updateOutput\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdateOutput\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Your code goes here. ################################################\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Linear transformation: output = input * W^T + b\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Your code goes here. ################################################\n",
    "# Flatten the input data\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_val_flat = X_val.reshape(X_val.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Define a function to create and train a model\n",
    "def train_model(model, criterion, optimizer_config, optimizer_func, X_train, y_train, X_val, y_val, n_epochs=10, batch_size=128):\n",
    "    optimizer_state = {}\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        for x_batch, y_batch in get_batches((X_train, y_train), batch_size):\n",
    "            model.zeroGradParameters()\n",
    "            \n",
    "            # Forward\n",
    "            predictions = model.forward(x_batch)\n",
    "            loss = criterion.forward(predictions, y_batch)\n",
    "            epoch_loss += loss\n",
    "            batch_count += 1\n",
    "            \n",
    "            # Backward\n",
    "            dp = criterion.backward(predictions, y_batch)\n",
    "            model.backward(x_batch, dp)\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer_func(model.getParameters(), \n",
    "                         model.getGradParameters(), \n",
    "                         optimizer_config,\n",
    "                         optimizer_state)\n",
    "        \n",
    "        train_loss = epoch_loss / batch_count\n",
    "        train_loss_history.append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.evaluate()\n",
    "        val_predictions = model.forward(X_val)\n",
    "        val_loss = criterion.forward(val_predictions, y_val)\n",
    "        val_loss_history.append(val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{n_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    return train_loss_history, val_loss_history\n",
    "\n",
    "# Define a function to calculate accuracy\n",
    "def calculate_accuracy(model, X, y):\n",
    "    model.evaluate()\n",
    "    predictions = model.forward(X)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predicted_classes == true_classes)\n",
    "    return accuracy\n",
    "\n",
    "# 1. Compare different activation functions\n",
    "input_size = 28 * 28\n",
    "hidden_size = 100\n",
    "output_size = 10\n",
    "\n",
    "# Create models with different activation functions\n",
    "def create_model(activation_class, *args):\n",
    "    model = Sequential()\n",
    "    model.add(Linear(input_size, hidden_size))\n",
    "    model.add(activation_class(*args))\n",
    "    model.add(Linear(hidden_size, output_size))\n",
    "    model.add(LogSoftMax())\n",
    "    return model\n",
    "\n",
    "# Initialize models\n",
    "relu_model = create_model(ReLU)\n",
    "elu_model = create_model(ELU, 1.0)\n",
    "leaky_relu_model = create_model(LeakyReLU, 0.01)\n",
    "softplus_model = create_model(SoftPlus)\n",
    "\n",
    "# Define criterion and optimizer config\n",
    "criterion = ClassNLLCriterion()\n",
    "optimizer_config = {'learning_rate': 0.01, 'momentum': 0.9}\n",
    "\n",
    "# Train models\n",
    "print(\"Training ReLU model...\")\n",
    "relu_train_loss, relu_val_loss = train_model(\n",
    "    relu_model, criterion, optimizer_config, sgd_momentum, \n",
    "    X_train_flat, y_train_one_hot, X_val_flat, y_val_one_hot, n_epochs=5\n",
    ")\n",
    "\n",
    "print(\"\\nTraining ELU model...\")\n",
    "elu_train_loss, elu_val_loss = train_model(\n",
    "    elu_model, criterion, optimizer_config, sgd_momentum, \n",
    "    X_train_flat, y_train_one_hot, X_val_flat, y_val_one_hot, n_epochs=5\n",
    ")\n",
    "\n",
    "print(\"\\nTraining LeakyReLU model...\")\n",
    "leaky_relu_train_loss, leaky_relu_val_loss = train_model(\n",
    "    leaky_relu_model, criterion, optimizer_config, sgd_momentum, \n",
    "    X_train_flat, y_train_one_hot, X_val_flat, y_val_one_hot, n_epochs=5\n",
    ")\n",
    "\n",
    "print(\"\\nTraining SoftPlus model...\")\n",
    "softplus_train_loss, softplus_val_loss = train_model(\n",
    "    softplus_model, criterion, optimizer_config, sgd_momentum, \n",
    "    X_train_flat, y_train_one_hot, X_val_flat, y_val_one_hot, n_epochs=5\n",
    ")\n",
    "\n",
    "# 2. Try BatchNormalization\n",
    "def create_model_with_bn(activation_class, *args):\n",
    "    model = Sequential()\n",
    "    model.add(Linear(input_size, hidden_size))\n",
    "    model.add(BatchNormalization(0.9))\n",
    "    model.add(ChannelwiseScaling(hidden_size))\n",
    "    model.add(activation_class(*args))\n",
    "    model.add(Linear(hidden_size, output_size))\n",
    "    model.add(LogSoftMax())\n",
    "    return model\n",
    "\n",
    "# Initialize model with BatchNormalization\n",
    "relu_bn_model = create_model_with_bn(ReLU)\n",
    "\n",
    "print(\"\\nTraining ReLU model with BatchNormalization...\")\n",
    "relu_bn_train_loss, relu_bn_val_loss = train_model(\n",
    "    relu_bn_model, criterion, optimizer_config, sgd_momentum, \n",
    "    X_train_flat, y_train_one_hot, X_val_flat, y_val_one_hot, n_epochs=5\n",
    ")\n",
    "\n",
    "# 3. Compare SGD with Adam\n",
    "# Create a new model for Adam\n",
    "relu_adam_model = create_model(ReLU)\n",
    "\n",
    "# Adam optimizer config\n",
    "adam_config = {'learning_rate': 0.001, 'beta1': 0.9, 'beta2': 0.999, 'epsilon': 1e-8}\n",
    "\n",
    "print(\"\\nTraining ReLU model with Adam optimizer...\")\n",
    "relu_adam_train_loss, relu_adam_val_loss = train_model(\n",
    "    relu_adam_model, criterion, adam_config, adam_optimizer, \n",
    "    X_train_flat, y_train_one_hot, X_val_flat, y_val_one_hot, n_epochs=5\n",
    ")\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot validation losses for different activation functions\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.semilogy(relu_val_loss, label='ReLU')\n",
    "plt.semilogy(elu_val_loss, label='ELU')\n",
    "plt.semilogy(leaky_relu_val_loss, label='LeakyReLU')\n",
    "plt.semilogy(softplus_val_loss, label='SoftPlus')\n",
    "plt.semilogy(relu_bn_val_loss, label='ReLU with BatchNorm')\n",
    "plt.title('Validation Loss Comparison (Activation Functions & BatchNorm)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot validation losses for SGD vs Adam\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.semilogy(relu_val_loss, label='ReLU with SGD')\n",
    "plt.semilogy(relu_adam_val_loss, label='ReLU with Adam')\n",
    "plt.title('Validation Loss Comparison (SGD vs Adam)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and print accuracies\n",
    "print(\"\\nAccuracies on validation set:\")\n",
    "print(f\"ReLU: {calculate_accuracy(relu_model, X_val_flat, y_val_one_hot):.4f}\")\n",
    "print(f\"ELU: {calculate_accuracy(elu_model, X_val_flat, y_val_one_hot):.4f}\")\n",
    "print(f\"LeakyReLU: {calculate_accuracy(leaky_relu_model, X_val_flat, y_val_one_hot):.4f}\")\n",
    "print(f\"SoftPlus: {calculate_accuracy(softplus_model, X_val_flat, y_val_one_hot):.4f}\")\n",
    "print(f\"ReLU with BatchNorm: {calculate_accuracy(relu_bn_model, X_val_flat, y_val_one_hot):.4f}\")\n",
    "print(f\"ReLU with Adam: {calculate_accuracy(relu_adam_model, X_val_flat, y_val_one_hot):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print here your accuracy on test set. It should be around 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training advanced model...\n",
      "Epoch 1/15, LR: 0.050000, Train Loss: 2.7226, Val Loss: 3.8539\n",
      "Epoch 2/15, LR: 0.050000, Train Loss: 37.0466, Val Loss: 36.7422\n",
      "Epoch 3/15, LR: 0.050000, Train Loss: 458.2231, Val Loss: 542.3616\n",
      "Epoch 4/15, LR: 0.005000, Train Loss: 1010.5544, Val Loss: 344.1253\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 84\u001b[0m\n\u001b[1;32m     81\u001b[0m criterion \u001b[38;5;241m=\u001b[39m ClassNLLCriterion()\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining advanced model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 84\u001b[0m advanced_train_loss, advanced_val_loss \u001b[38;5;241m=\u001b[39m train_model_with_lr_decay(\n\u001b[1;32m     85\u001b[0m     advanced_model, criterion, sgd_momentum, \n\u001b[1;32m     86\u001b[0m     X_train_flat, y_train_one_hot, X_val_flat, y_val_one_hot,\n\u001b[1;32m     87\u001b[0m     initial_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, n_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m\n\u001b[1;32m     88\u001b[0m )\n",
      "Cell \u001b[0;32mIn[37], line 34\u001b[0m, in \u001b[0;36mtrain_model_with_lr_decay\u001b[0;34m(model, criterion, optimizer_func, X_train, y_train, X_val, y_val, initial_lr, momentum, n_epochs, batch_size)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[1;32m     33\u001b[0m dp \u001b[38;5;241m=\u001b[39m criterion\u001b[38;5;241m.\u001b[39mbackward(predictions, y_batch)\n\u001b[0;32m---> 34\u001b[0m model\u001b[38;5;241m.\u001b[39mbackward(x_batch, dp)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[1;32m     37\u001b[0m optimizer_func(model\u001b[38;5;241m.\u001b[39mgetParameters(), \n\u001b[1;32m     38\u001b[0m              model\u001b[38;5;241m.\u001b[39mgetGradParameters(), \n\u001b[1;32m     39\u001b[0m              optimizer_config,\n\u001b[1;32m     40\u001b[0m              optimizer_state)\n",
      "File \u001b[0;32m/var/folders/d3/zcslj2712p57dfxhz77sfh8r0000gn/T/ipykernel_47963/975152187.py:84\u001b[0m, in \u001b[0;36mSequential.backward\u001b[0;34m(self, input, gradOutput)\u001b[0m\n\u001b[1;32m     82\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules[i]\n\u001b[1;32m     83\u001b[0m     module_input \u001b[38;5;241m=\u001b[39m intermediate_inputs[i]\n\u001b[0;32m---> 84\u001b[0m     current_grad \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mbackward(module_input, current_grad)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradInput \u001b[38;5;241m=\u001b[39m current_grad\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradInput\n",
      "File \u001b[0;32m/var/folders/d3/zcslj2712p57dfxhz77sfh8r0000gn/T/ipykernel_47963/1342133365.py:34\u001b[0m, in \u001b[0;36mModule.backward\u001b[0;34m(self, input, gradOutput)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m,\u001b[38;5;28minput\u001b[39m, gradOutput):\n\u001b[1;32m     27\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    Performs a backpropagation step through the module, with respect to the given input.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m     - computing a gradient w.r.t. parameters (to update parameters while optimizing).\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdateGradInput(\u001b[38;5;28minput\u001b[39m, gradOutput)\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccGradParameters(\u001b[38;5;28minput\u001b[39m, gradOutput)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradInput\n",
      "File \u001b[0;32m/var/folders/d3/zcslj2712p57dfxhz77sfh8r0000gn/T/ipykernel_47963/908022283.py:52\u001b[0m, in \u001b[0;36mBatchNormalization.updateGradInput\u001b[0;34m(self, input, gradOutput)\u001b[0m\n\u001b[1;32m     49\u001b[0m dx_normalized \u001b[38;5;241m=\u001b[39m gradOutput\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Gradient with respect to variance\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m dvar \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(dx_normalized \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_centered \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mpower(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_var \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mEPS, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.5\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Gradient with respect to mean\u001b[39;00m\n\u001b[1;32m     55\u001b[0m dmean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(dx_normalized \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstd, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m dvar \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_centered, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:2313\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2310\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   2311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m-> 2313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _wrapreduction(a, np\u001b[38;5;241m.\u001b[39madd, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m, axis, dtype, out, keepdims\u001b[38;5;241m=\u001b[39mkeepdims,\n\u001b[1;32m   2314\u001b[0m                       initial\u001b[38;5;241m=\u001b[39minitial, where\u001b[38;5;241m=\u001b[39mwhere)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:88\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Your code goes here. ################################################\n",
    "# Define a function for learning rate decay\n",
    "def learning_rate_decay(initial_lr, epoch, decay_rate=0.1, decay_steps=3):\n",
    "    return initial_lr * (decay_rate ** (epoch // decay_steps))\n",
    "\n",
    "# Define a function to train with learning rate decay\n",
    "def train_model_with_lr_decay(model, criterion, optimizer_func, X_train, y_train, X_val, y_val, \n",
    "                             initial_lr=0.01, momentum=0.9, n_epochs=15, batch_size=128):\n",
    "    optimizer_state = {}\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Update learning rate\n",
    "        current_lr = learning_rate_decay(initial_lr, epoch)\n",
    "        optimizer_config = {'learning_rate': current_lr, 'momentum': momentum}\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        for x_batch, y_batch in get_batches((X_train, y_train), batch_size):\n",
    "            model.zeroGradParameters()\n",
    "            \n",
    "            # Forward\n",
    "            predictions = model.forward(x_batch)\n",
    "            loss = criterion.forward(predictions, y_batch)\n",
    "            epoch_loss += loss\n",
    "            batch_count += 1\n",
    "            \n",
    "            # Backward\n",
    "            dp = criterion.backward(predictions, y_batch)\n",
    "            model.backward(x_batch, dp)\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer_func(model.getParameters(), \n",
    "                         model.getGradParameters(), \n",
    "                         optimizer_config,\n",
    "                         optimizer_state)\n",
    "        \n",
    "        train_loss = epoch_loss / batch_count\n",
    "        train_loss_history.append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.evaluate()\n",
    "        val_predictions = model.forward(X_val)\n",
    "        val_loss = criterion.forward(val_predictions, y_val)\n",
    "        val_loss_history.append(val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{n_epochs}, LR: {current_lr:.6f}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    return train_loss_history, val_loss_history\n",
    "\n",
    "# Build a more advanced model\n",
    "def create_advanced_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First layer\n",
    "    model.add(Linear(input_size, 256))\n",
    "    model.add(BatchNormalization(0.9))\n",
    "    model.add(ChannelwiseScaling(256))\n",
    "    model.add(LeakyReLU(0.01))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Second layer\n",
    "    model.add(Linear(256, 128))\n",
    "    model.add(BatchNormalization(0.9))\n",
    "    model.add(ChannelwiseScaling(128))\n",
    "    model.add(LeakyReLU(0.01))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Linear(128, output_size))\n",
    "    model.add(LogSoftMax())\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and train the advanced model\n",
    "advanced_model = create_advanced_model()\n",
    "criterion = ClassNLLCriterion()\n",
    "\n",
    "print(\"Training advanced model...\")\n",
    "advanced_train_loss, advanced_val_loss = train_model_with_lr_decay(\n",
    "    advanced_model, criterion, sgd_momentum, \n",
    "    X_train_flat, y_train_one_hot, X_val_flat, y_val_one_hot,\n",
    "    initial_lr=0.05, momentum=0.9, n_epochs=15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing with PyTorch implementation\n",
    "The last (and maybe the easiest) step after compared to the previous tasks: build a network with the same architecture as above now with PyTorch.\n",
    "\n",
    "__Good Luck!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your beautiful code here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
