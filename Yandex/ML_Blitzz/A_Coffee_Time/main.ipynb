{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import logsumexp # For numerically stable log(1+exp(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(file_path=\"restaurants_train.txt\"):\n",
    "    training_data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line_number, line in enumerate(f, 1):\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 5:\n",
    "                winner = float(parts[0])\n",
    "                r1 = max(float(parts[1]), 0)\n",
    "                r2 = max(float(parts[2]), 0)\n",
    "                d1 = max(float(parts[3]), 0)\n",
    "                d2 = max(float(parts[4]), 0)\n",
    "                training_data.append({'winner': winner, 'r1': r1, 'd1': d1, 'r2': r2, 'd2': d2})\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_function(params, r, d):\n",
    "    \"\"\"\n",
    "    Calculates the goodness score for a coffee shop.\n",
    "    S(r,d) = wr * r - wd * d + b\n",
    "    \"\"\"\n",
    "    wr, wd, b = params\n",
    "    return wr * r - wd * d + b\n",
    "\n",
    "# 2. Loss Function for Training\n",
    "def loss_function(params, training_data_list):\n",
    "    \"\"\"\n",
    "    Calculates the average logistic loss over the training dataset.\n",
    "    This is the performance metric 'm' to be minimized.\n",
    "    \"\"\"\n",
    "    total_loss = 0.0\n",
    "    num_comparisons = len(training_data_list)\n",
    "\n",
    "    if num_comparisons == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # wr, wd, b = params # Parameters are directly passed\n",
    "    # Monotonicity constraints (wr >= 0, wd >= 0) are handled by the optimizer's bounds.\n",
    "\n",
    "    for comp in training_data_list:\n",
    "        s1 = score_function(params, comp['r1'], comp['d1'])\n",
    "        s2 = score_function(params, comp['r2'], comp['d2'])\n",
    "        winner_label = comp['winner']\n",
    "\n",
    "        if winner_label == 0.0:  # Shop 1 preferred\n",
    "            # Loss term: log(1 + exp(s2 - s1))\n",
    "            term = s2 - s1\n",
    "            total_loss += logsumexp([0, term]) # Numerically stable version\n",
    "        elif winner_label == 1.0:  # Shop 2 preferred\n",
    "            # Loss term: log(1 + exp(s1 - s2))\n",
    "            term = s1 - s2\n",
    "            total_loss += logsumexp([0, term]) # Numerically stable version\n",
    "        elif winner_label == 0.5:  # Draw\n",
    "            # Loss term: 0.5 * log(1 + exp(s2 - s1)) + 0.5 * log(1 + exp(s1 - s2))\n",
    "            term1 = s2 - s1\n",
    "            term2 = s1 - s2\n",
    "            loss_draw = 0.5 * logsumexp([0, term1]) + 0.5 * logsumexp([0, term2])\n",
    "            total_loss += loss_draw\n",
    "        # else: Malformed winner label, skip or error. Assuming valid input.\n",
    "            \n",
    "    return total_loss / num_comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Model Training\n",
    "def train_model(training_data_list):\n",
    "    \"\"\"\n",
    "    Trains the linear model by minimizing the loss function.\n",
    "    Sets the global optimal_wr, optimal_wd, optimal_b.\n",
    "    \"\"\"\n",
    "    global optimal_wr, optimal_wd, optimal_b\n",
    "    \n",
    "    if not training_data_list:\n",
    "        print(\"Error: No training data provided to train_model.\")\n",
    "        # Set default parameters or handle as failure\n",
    "        optimal_wr, optimal_wd, optimal_b = 1.0, 1.0, 0.0 # Default/fallback parameters\n",
    "        print(\"Warning: Using default parameters as training data is empty.\")\n",
    "        return\n",
    "\n",
    "    # Initial guesses for wr, wd, b\n",
    "    initial_params = np.array([1.0, 1.0, 0.0])  \n",
    "    \n",
    "    # Bounds for parameters: wr >= 0, wd >= 0, b is unbounded\n",
    "    bounds = [(0, None), (0, None), (None, None)] \n",
    "\n",
    "    # Minimize the loss function using L-BFGS-B\n",
    "    result = minimize(loss_function, \n",
    "                      initial_params, \n",
    "                      args=(training_data_list,), \n",
    "                      method='L-BFGS-B', \n",
    "                      bounds=bounds,\n",
    "                      options={'maxiter': 1000, 'disp': False, 'ftol': 1e-9, 'gtol': 1e-7}) # Standard options\n",
    "\n",
    "    if result.success:\n",
    "        optimal_params = result.x\n",
    "        optimal_wr, optimal_wd, optimal_b = optimal_params[0], optimal_params[1], optimal_params[2]\n",
    "        print(f'{optimal_wr:.10f}, {optimal_wd:.10f}, {optimal_b:.10f}')\n",
    "        # Monotonicity check (L-BFGS-B with bounds should ensure this)\n",
    "        # Allowing for very small negative values due to numerical precision.\n",
    "        if optimal_wr < -1e-6 or optimal_wd < -1e-6:\n",
    "             # This is critical as non-monotonic solutions get 0 points.\n",
    "             print(f\"CRITICAL ERROR: Monotonicity violated by learned weights: wr={optimal_wr:.4f}, wd={optimal_wd:.4f}.\")\n",
    "             print(\"This solution would receive 0 points. Aborting.\")\n",
    "             exit(1) # Abort if monotonicity is violated\n",
    "        \n",
    "        # print(f\"Training successful. Optimal parameters: wr={optimal_wr:.4f}, wd={optimal_wd:.4f}, b={optimal_b:.4f}\")\n",
    "        # print(f\"Minimized loss (m): {result.fun:.6f}\")\n",
    "    else:\n",
    "        print(f\"Error: Optimization failed to converge. Message: {result.message}\")\n",
    "        # Fallback to initial or default parameters if optimization fails.\n",
    "        # For this problem, successful optimization is crucial.\n",
    "        optimal_wr, optimal_wd, optimal_b = initial_params[0], initial_params[1], initial_params[2]\n",
    "        print(f\"Warning: Using initial parameters due to optimization failure: wr={optimal_wr}, wd={optimal_wd}, b={optimal_b}\")\n",
    "        # Depending on requirements, might need to exit(1) here too.\n",
    "        # For now, allow scoring with initial params but flag it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_wr = 0.0\n",
    "optimal_wd = 0.0\n",
    "optimal_b = 0.0    \n",
    "\n",
    "training_file_name = \"restaurants_train.txt\"\n",
    "# test_input_file_name = \"restaurants.in\"\n",
    "\n",
    "# Step 1: Load training data\n",
    "training_data_list = load_training_data(training_file_name)\n",
    "print(len(training_data_list))\n",
    "print(training_data_list[0])\n",
    "\n",
    "# train model:\n",
    "train_model(training_data_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_wr, optimal_wd, optimal_b = 0.1301726308, 1.5460590234, 0.0000000210\n",
    "# print current loss\n",
    "print(f'{loss_function([optimal_wr, optimal_wd, optimal_b], training_data_list) - 0.6575930758932114:.10f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_stop_condition(training_data_list, params):\n",
    "    for comp in training_data_list:\n",
    "        if comp['r1'] > 0 and comp['r2'] > 0:\n",
    "            winner = comp['r1'] > comp['r2'] and comp['d1'] < comp['d2']\n",
    "            if winner: \n",
    "                print(comp['r1'] > comp['r2'], comp['d1'] < comp['d2'], winner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_stop_condition(training_data_list, [optimal_wr, optimal_wd, optimal_b])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
