{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDsVMGiVgSq2"
      },
      "source": [
        "## Переобучение нейронных сетей и борьба с ним\n",
        "\n",
        "##### Автор: [Радослав Нейчев](https://www.linkedin.com/in/radoslav-neychev/), https://t.me/girafe_ai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3isBRG6PgSq6"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "from IPython.display import clear_output\n",
        "from matplotlib import pyplot as plt\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torchvision.datasets import FashionMNIST\n",
        "\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtkvHPeOsRWg"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "\n",
        "def args_and_kwargs(*args, **kwargs):\n",
        "    return args, kwargs\n",
        "\n",
        "def parse_pytorch_model(model_str):\n",
        "    def parse_layer(layer_str):\n",
        "        layer_name, params = layer_str.split(\"(\", 1)\n",
        "        layer_info = {\"type\": layer_name.strip()}\n",
        "        params_template = layer_str.replace(layer_name, \"args_and_kwargs\")\n",
        "\n",
        "        param_dict = {}\n",
        "        if len(params):\n",
        "            args, kwargs = eval(params_template)\n",
        "            if len(args) or len(kwargs):\n",
        "                param_dict[\"args\"] = args\n",
        "                for name, value in kwargs.items():\n",
        "                    param_dict[name] = value\n",
        "        layer_info[\"parameters\"] = param_dict\n",
        "        return layer_info\n",
        "\n",
        "    model_dict = {}\n",
        "    lines = model_str.splitlines()\n",
        "    model_name = lines[0].strip(\"()\")\n",
        "    model_dict[\"model_name\"] = model_name\n",
        "    model_dict[\"layers\"] = []\n",
        "\n",
        "    layer_regex = re.compile(r\"\\((\\d+)\\): (.+)\")\n",
        "    for line in lines[1:]:\n",
        "        line = line.strip()\n",
        "        match = layer_regex.match(line)\n",
        "        if match:\n",
        "            index, layer = match.groups()\n",
        "            model_dict[\"layers\"].append({\"index\": int(index), \"layer\": parse_layer(layer)})\n",
        "    return model_dict\n",
        "\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rghGdcWEsRWg"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "def get_predictions(model, eval_data, step=10):\n",
        "\n",
        "    predicted_labels = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for idx in range(0, len(eval_data), step):\n",
        "            y_predicted = model(eval_data[idx : idx + step].to(device))\n",
        "            predicted_labels.append(y_predicted.argmax(dim=1).cpu())\n",
        "\n",
        "    predicted_labels = torch.cat(predicted_labels)\n",
        "    predicted_labels = \",\".join([str(x.item()) for x in list(predicted_labels)])\n",
        "    return predicted_labels\n",
        "\n",
        "\n",
        "def get_accuracy(model, data_loader):\n",
        "    predicted_labels = []\n",
        "    real_labels = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            y_predicted = model(batch[0].to(device))\n",
        "            predicted_labels.append(y_predicted.argmax(dim=1).cpu())\n",
        "            real_labels.append(batch[1])\n",
        "\n",
        "    predicted_labels = torch.cat(predicted_labels)\n",
        "    real_labels = torch.cat(real_labels)\n",
        "    accuracy_score = (predicted_labels == real_labels).type(torch.FloatTensor).mean()\n",
        "    return accuracy_score\n",
        "\n",
        "\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcYVLIcosRWg"
      },
      "source": [
        "Загрузите файл `hw_overfitting_data_dict.npy` (ссылка есть на странице с заданием), он понадобится для генерации посылок. Код ниже может его загрузить (но в случае возникновения ошибки скачайте и загрузите его вручную).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lA7JHHl5sRWh",
        "outputId": "ae771174-8114-4970-89d5-a9cfc6bbbc50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-19 15:37:05--  https://github.com/girafe-ai/ml-course/raw/24f_ysda/homeworks/hw_overfitting/hw_overfitting_data_dict\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/girafe-ai/ml-course/24f_ysda/homeworks/hw_overfitting/hw_overfitting_data_dict [following]\n",
            "--2025-04-19 15:37:06--  https://raw.githubusercontent.com/girafe-ai/ml-course/24f_ysda/homeworks/hw_overfitting/hw_overfitting_data_dict\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6272446 (6.0M) [application/octet-stream]\n",
            "Saving to: ‘hw_overfitting_data_dict.npy’\n",
            "\n",
            "hw_overfitting_data 100%[===================>]   5.98M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-04-19 15:37:06 (172 MB/s) - ‘hw_overfitting_data_dict.npy’ saved [6272446/6272446]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/girafe-ai/ml-course/raw/24f_ysda/homeworks/hw_overfitting/hw_overfitting_data_dict -O hw_overfitting_data_dict.npy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TeIPa6ssRWh"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "assert os.path.exists(\n",
        "    \"hw_overfitting_data_dict.npy\"\n",
        "), \"Please, download `hw_overfitting_data_dict.npy` and place it in the working directory\"\n",
        "\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeA6Q5-CgSq7"
      },
      "source": [
        "### Задача №1 (уже решённая): Создание и обучение модели (Separation)\n",
        "Вы уже решали эту задачу ранее, так что сейчас просто воспроизведите своё решение. Оно понадобится вам в дальнейших шагах.\n",
        "__Ваша первая задача всё та же: реализовать весь пайплан обучения модели и добиться качества $\\geq 88.5\\%$ на тестовой выборке.__\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_bjRZ7PsRWh"
      },
      "outputs": [],
      "source": [
        "CUDA_DEVICE_ID = 0  # change if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPG1KbQAgl8b"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "device = (\n",
        "    torch.device(f\"cuda:{CUDA_DEVICE_ID}\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        ")\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "aYcL28OsgSq8",
        "outputId": "c33f43bc-fe7e-49f5-ddec-a1051d4ba0c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:00<00:00, 115MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 3.58MB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 41.3MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 9.30MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Image label: 8')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGzCAYAAABpdMNsAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAKKNJREFUeJzt3Xt0VOXd9//P5DQBkgwGJAcIGCKCioKiIh4QgZLEpYLwE1H7CNRCxcAtUKxiFURbcxcttSrqWj2Q+pODt3c5eKi0CgSqAhaUgm2lgOFMgokmEwIJSeZ6/uBh6kACXGPClYT3a629FrPn+s7+zs4On+zZO1c8xhgjAADOsgjXDQAAzk0EEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEHCW7dy5Ux6PR3l5eda1Tz75pDwej4qLixusnzFjxuiCCy5osNcDzhQBhCYlLy9PHo9HGzZscN0KzlBlZaVyc3N1ySWXqHXr1urYsaPuvPNO/eMf/3DdGpq4KNcNAGje7r33Xr311lsaN26crrzySu3fv19z585Vv379tGXLFnXp0sV1i2iiCCAAYdu3b58WL16sadOm6dlnnw2uv/HGGzVw4EAtXrxYU6ZMcdghmjI+gkOTN2bMGMXFxWn37t269dZbFRcXp44dO2ru3LmSpC1btmjgwIFq06aNunTpogULFoTUf/3115o2bZouu+wyxcXFKSEhQdnZ2fr73/9+0rZ27dql22+/XW3atFGHDh00ZcoU/fnPf5bH41F+fn7I2PXr1ysrK0s+n0+tW7fWTTfdpI8++iis97h582aNGTNGXbt2VWxsrJKTk/WDH/xAJSUldY4vLi7WyJEjlZCQoHbt2umhhx5SZWXlSeNef/119enTR61atVJiYqJGjRqlPXv2nLafAwcO6IsvvlB1dfUpx5WXl0uSkpKSQtanpKRIklq1anXabeHcRQChWaitrVV2drbS0tI0e/ZsXXDBBZo4caLy8vKUlZWlq666Sr/4xS8UHx+v++67TwUFBcHaL7/8UkuXLtWtt96qOXPm6OGHH9aWLVt00003af/+/cFxFRUVGjhwoD744AP913/9l37605/q448/1iOPPHJSPytXrlT//v3l9/s1c+ZMPfPMMyotLdXAgQP1ySefWL+/999/X19++aXGjh2rF198UaNGjdKiRYt0yy23qK6/mDJy5MjgtZdbbrlFL7zwgsaPHx8y5uc//7nuu+8+devWTXPmzNHkyZO1YsUK9e/fX6WlpafsZ/r06br44ou1b9++U47LyMhQp06d9Mtf/lJvv/229u7dq08++UQPPPCA0tPTNWrUKOt9gXOIAZqQefPmGUnmb3/7W3Dd6NGjjSTzzDPPBNd98803plWrVsbj8ZhFixYF13/xxRdGkpk5c2ZwXWVlpamtrQ3ZTkFBgfF6veapp54KrvvlL39pJJmlS5cG1x05csT06NHDSDKrVq0yxhgTCARMt27dTGZmpgkEAsGxhw8fNunp6eZ73/veKd9jQUGBkWTmzZsXUnuihQsXGklmzZo1wXUzZ840ksztt98eMvbBBx80kszf//53Y4wxO3fuNJGRkebnP/95yLgtW7aYqKiokPWjR482Xbp0CRl3fJ8XFBSc8r0YY8z69etNRkaGkRRc+vTpYw4cOHDaWpzbOANCs/HDH/4w+O+2bduqe/fuatOmjUaOHBlc3717d7Vt21ZffvllcJ3X61VExLFDvba2ViUlJYqLi1P37t316aefBsctX75cHTt21O233x5cFxsbq3HjxoX0sWnTJm3btk333HOPSkpKVFxcrOLiYlVUVGjQoEFas2aNAoGA1Xv79kdVlZWVKi4u1rXXXitJIT0el5OTE/J40qRJkqQ//elPkqTFixcrEAho5MiRwf6Ki4uVnJysbt26adWqVafsJy8vT8aYM7o9+7zzzlPv3r316KOPaunSpXruuee0c+dO3XnnnXV+LAgcx00IaBZiY2N1/vnnh6zz+Xzq1KmTPB7PSeu/+eab4ONAIKBf//rXevnll1VQUKDa2trgc+3atQv+e9euXcrIyDjp9S688MKQx9u2bZMkjR49ut5+y8rKdN55553huzt2nWrWrFlatGiRDh48eNJrnahbt24hjzMyMhQREaGdO3cGezTGnDTuuOjo6DPu7VTKysp044036uGHH9aPf/zj4PqrrrpKAwYM0Lx58zRhwoQG2RZaHgIIzUJkZKTVevOt6ybPPPOMnnjiCf3gBz/Q008/rcTEREVERGjy5MnWZyqSgjXPPvusevfuXeeYuLg4q9ccOXKkPv74Yz388MPq3bu34uLiFAgElJWVdUY9nhiagUBAHo9H7733Xp37yLa/+vzxj39UUVFRyFmjJN10001KSEjQRx99RAChXgQQWrz//d//1c0336zf/e53IetLS0vVvn374OMuXbron//8p4wxIf+hb9++PaQuIyNDkpSQkKDBgwd/5/6++eYbrVixQrNmzdKMGTOC64+fadVl27ZtSk9PD+kxEAgEPzLLyMiQMUbp6em66KKLvnOP9SkqKpKkkLNK6dgPALW1taqpqWm0baP54xoQWrzIyMiT7iR78803T7rDKzMzU/v27dNbb70VXFdZWanf/OY3IeP69OmjjIwMPffcczp06NBJ2/vqq6+s+5N0Uo/PP/98vTXHb0E/7sUXX5QkZWdnS5KGDx+uyMhIzZo166TXNcbUe3v3cWd6G/bxcFu0aFHI+rfeeksVFRW64oorTlmPcxtnQGjxbr31Vj311FMaO3asrrvuOm3ZskXz589X165dQ8b96Ec/0ksvvaS7775bDz30kFJSUjR//nzFxsZK+s/HXBEREfrtb3+r7OxsXXrppRo7dqw6duyoffv2adWqVUpISNDbb799xv0lJCSof//+mj17tqqrq9WxY0f95S9/CbmV/EQFBQW6/fbblZWVpbVr1+r111/XPffco169ekk6dgb0s5/9TNOnT9fOnTs1bNgwxcfHq6CgQEuWLNH48eM1bdq0el9/+vTp+sMf/qCCgoJT3ohw22236dJLL9VTTz2lXbt26dprr9X27dv10ksvKSUlRffff/8Z7wecewggtHiPPfaYKioqtGDBAr3xxhu68sor9e677+rRRx8NGRcXF6eVK1dq0qRJ+vWvf624uDjdd999uu666zRixIhgEEnSgAEDtHbtWj399NN66aWXdOjQISUnJ6tv37760Y9+ZN3jggULNGnSJM2dO1fGGA0ZMkTvvfeeUlNT6xz/xhtvaMaMGXr00UcVFRWliRMnhsxEIEmPPvqoLrroIv3qV7/SrFmzJElpaWkaMmTISddswhUTE6O//vWvevrpp/Xuu+9q4cKFio+P17Bhw/TMM8+EfMQJnMhjTjw/BxDi+eef15QpU7R371517NjRdTtAi0EAAd9y5MiRk34n54orrlBtba3+/e9/O+wMaHn4CA74luHDh6tz587q3bu3ysrK9Prrr+uLL77Q/PnzXbcGtDgEEPAtmZmZ+u1vf6v58+ertrZWl1xyiRYtWqS77rrLdWtAi8NHcAAAJ/g9IACAEwQQAMCJJncNKBAIaP/+/YqPjz9pfisAQNNnjFF5eblSU1ODM9HXpckF0P79+5WWlua6DQDAd7Rnzx516tSp3uebXADFx8dLkm7QLYpSw0wZDwA4e2pUrQ/1p+D/5/VptACaO3eunn32WRUWFqpXr1568cUXdc0115y27vjHblGKVpSHAAKAZuf/3Vt9ussojXITwhtvvKGpU6dq5syZ+vTTT9WrVy9lZmae9Ie2AADnrkYJoDlz5mjcuHEaO3asLrnkEr366qtq3bq1fv/73zfG5gAAzVCDB9DRo0e1cePGkD/UFRERocGDB2vt2rUnja+qqpLf7w9ZAAAtX4MHUHFxsWpra5WUlBSyPikpSYWFhSeNz83Nlc/nCy7cAQcA5wbnv4g6ffp0lZWVBZc9e/a4bgkAcBY0+F1w7du3V2RkZPBvxR9XVFSk5OTkk8Z7vV55vd6GbgMA0MQ1+BlQTEyM+vTpoxUrVgTXBQIBrVixQv369WvozQEAmqlG+T2gqVOnavTo0brqqqt0zTXX6Pnnn1dFRYXGjh3bGJsDADRDjRJAd911l7766ivNmDFDhYWF6t27t5YvX37SjQkAgHNXk/t7QH6/Xz6fTwM0lJkQAKAZqjHVytcylZWVKSEhod5xzu+CAwCcmwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcaPAAevLJJ+XxeEKWHj16NPRmAADNXFRjvOill16qDz744D8biWqUzQAAmrFGSYaoqCglJyc3xksDAFqIRrkGtG3bNqWmpqpr16669957tXv37nrHVlVVye/3hywAgJavwQOob9++ysvL0/Lly/XKK6+ooKBAN954o8rLy+scn5ubK5/PF1zS0tIauiUAQBPkMcaYxtxAaWmpunTpojlz5uj+++8/6fmqqipVVVUFH/v9fqWlpWmAhirKE92YrQEAGkGNqVa+lqmsrEwJCQn1jmv0uwPatm2riy66SNu3b6/zea/XK6/X29htAACamEb/PaBDhw5px44dSklJaexNAQCakQYPoGnTpmn16tXauXOnPv74Y91xxx2KjIzU3Xff3dCbAgA0Yw3+EdzevXt19913q6SkROeff75uuOEGrVu3Tueff35DbwoA0Iw1eAAtWrSooV8SANACMRccAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4ESU6wYayoGp11nXHG1rwtpWx9VHrWuiVmwMa1u2Km+7xrrmwHWRYW2rukO1dU10a/uaGG+NdY3HE97X1hhPGDX22/HYbyas7VQfDe9bPBCw/9k0UB3Gz7MV9v213m1/vMYWh3c8dFj2b+ua2uIS65rIizKsa/ZnJlnXSFKrkoB1TcKCdWFt63Q4AwIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJ5rsZKS1/XvJExV7xuNfmviy9TZiVGtdI0mb70yzromU/QSAadH2kxoW1uy1rvnI3826RpL2HW5rXVNRHWNdExHGxKJVNeEd2uFsKxDGBKbhbCc6MrzjNRzhvKc20faT9CbFllvXdLrhG+uam+P+ZV0jSfGPV1rXvFQ0yLrmglb2/fVt8451jSTFRxyxrpl54IdW42trKqU1y047jjMgAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCiyU5G6u/iVWSM94zHVwairbdRrjOf7PTb1pd1ta6pCtjv6rFJf7Wu6R1rPxnpBTHF1jVSePs8HOWBVtY1pbWtw9pWbES1fY3HvqbS2O+7isCZfz8cVxXm16jS2B+vrSPsJyMNGPufgb+uaWNd8+mRC6xrJCkx6pB1zaaijtY1Fe3sJ+nt1Xq3dY0klVTHWdfUeiPtxkec2XjOgAAAThBAAAAnrANozZo1uu2225SamiqPx6OlS5eGPG+M0YwZM5SSkqJWrVpp8ODB2rZtW0P1CwBoIawDqKKiQr169dLcuXPrfH727Nl64YUX9Oqrr2r9+vVq06aNMjMzVVlp/4edAAAtl/WVxuzsbGVnZ9f5nDFGzz//vB5//HENHTpUkvTaa68pKSlJS5cu1ahRo75btwCAFqNBrwEVFBSosLBQgwcPDq7z+Xzq27ev1q5dW2dNVVWV/H5/yAIAaPkaNIAKCwslSUlJSSHrk5KSgs+dKDc3Vz6fL7ikpaU1ZEsAgCbK+V1w06dPV1lZWXDZs2eP65YAAGdBgwZQcnKyJKmoqChkfVFRUfC5E3m9XiUkJIQsAICWr0EDKD09XcnJyVqxYkVwnd/v1/r169WvX7+G3BQAoJmzvgvu0KFD2r59e/BxQUGBNm3apMTERHXu3FmTJ0/Wz372M3Xr1k3p6el64oknlJqaqmHDhjVk3wCAZs46gDZs2KCbb745+Hjq1KmSpNGjRysvL08/+clPVFFRofHjx6u0tFQ33HCDli9frtjY8OZdAwC0TB5jjHHdxLf5/X75fD4N0FBFec58IsXDw/tabyv7yXzrGkkqPGp/naq4yn4CwPho+1/ejfTYfznPjym3rpGk86IqrGviI+zfUzgThIYrPuLIWdlOdRiTfcaGMdnn+ZHhfW3D8XWt/TF+1NhNcilJ26vqvp58KutK061rJCk51v7XQj4ttr+T9+Lzik4/6ARTk963rpGkyaMesC9at9lqeI2pVr6Wqays7JTX9Z3fBQcAODcRQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgRIuZDTscUR1Tw6pLX1piXfP/Jf7Nuubdsl7WNaXVra1rKmpirGskyV9t/yc2IsKYrTscAeMJqy420n7m7UPVXusaE0Z/Ncb+58XaQHg/Y0ZH1lrXeCNrrGtqwuivfaz9LOxJXvtZrSWpVRjHw0WxB6xr/vJ1T+uar24Ic6bzgP3X1hazYQMAmjQCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOBHluoF6eTzHljMVxpyqNfv2W9dI0rar7WvGL/q+dc3jvf9kXbP04BXWNb18+6xrJCk+stK65lN/Z+ua0qOtrGsOHbWfIFSSjkQ07gS4x4UzSWhljf23a7iTkdYG7CdL9UbbT0aa0sZ+ktBubQ5a12w9lGRdI0ltY45Y19zXodi65jePdbeuaR1Yb10jSRGt7ScsDhw+HNa2ToczIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwoulORmqMJPsJRm14omPCqjPVR61rojfHWdcEetv/fNDOaz9p4KHa8CbuHBz/uXVNZtw/rGviI2qta1rbTGT7LZGyr4sIY1vlAfv3VBXGt4M3vN2gNh77Y6/CBKxrygKR1jWxHvt9N2TzJOsaSXri6netaw7WVljXJHxWaF1jP/XrMY01sWg4OAMCADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACc8xpjGnfHTkt/vl8/n0wANVZQnunE3FuaElQpnl0XYT7r4m52rrWueO3izdc3OinbWNZJUUW0/mWt1rf1+8EbZT7sY6bGfGFOSDofxnhK8lfY10fY13kj7/XCktpG/h74lYMKZyNX+eyk51m9d86/SZOsaSTpQHm9dc2iXz7qmzT77c4HU2R9b10iSx2s/+bCpqrIaX2Oqla9lKisrU0JCQr3jOAMCADhBAAEAnLAOoDVr1ui2225TamqqPB6Pli5dGvL8mDFj5PF4QpasrKyG6hcA0EJYB1BFRYV69eqluXPn1jsmKytLBw4cCC4LFy78Tk0CAFoe67+Imp2drezs7FOO8Xq9Sk4O76IfAODc0CjXgPLz89WhQwd1795dEyZMUElJSb1jq6qq5Pf7QxYAQMvX4AGUlZWl1157TStWrNAvfvELrV69WtnZ2aqtrfvvuOfm5srn8wWXtLS0hm4JANAEWX8EdzqjRo0K/vuyyy7T5ZdfroyMDOXn52vQoEEnjZ8+fbqmTp0afOz3+wkhADgHNPpt2F27dlX79u21ffv2Op/3er1KSEgIWQAALV+jB9DevXtVUlKilJSUxt4UAKAZsf4I7tChQyFnMwUFBdq0aZMSExOVmJioWbNmacSIEUpOTtaOHTv0k5/8RBdeeKEyMzMbtHEAQPNmHUAbNmzQzTf/Z76x49dvRo8erVdeeUWbN2/WH/7wB5WWlio1NVVDhgzR008/LW8Y8w8BAFou6wAaMGCATjV/6Z///Ofv1NBZdTbnYQ3UfRfgqdx/7yTrmtfmv2hd89DuodY1klQTsP8E9zzvYeuacCa5DFcPX5F1zXlR9u/pm5rW1jU1AfuJXHvF77GukaSDR+2vxX7uT7WuaeetsK4JZ4LVly9cZF0jSf/nkWnWNT0mbrOu2ba1u3VNuDxhTMLcWP9TMhccAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnGjwP8mNeoQxA23EXz+zrsl+9ifWNQ8+uNS6RpL+edh+9uPCSvtZlnf5z7OuKatoZV0jSTdf/G/rmj/uu8K6Zve/k6xrTKz9jOoDesZY10hSjzaF1jWxkdXWNfFRldY1F8SWWNfsrPFZ10hS/KJ11jW7ovtZ13T4/z+2rglXoKrqrG3rdDgDAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnPMYY47qJb/P7/fL5fBqgoYryRLtu55xwNPOqsOp8j+2xrpnc6X3rmvWHM6xrDgfCm4SzdcRR65p070HrmkjZf9uNiPNb14SrythPLPpIof0knBmxX1nXRHtqrGveurG7dY0k1ZZ8HVadLU+0/fFqqu2P1bOlxlQrX8tUVlamhIT6JyDmDAgA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnGAy0qbM47GvaVpfzpNEtvVZ1wQuTLOuqUxqZV0jSTWxYfxMFs6XKcK+KKLa/msb+/XZm7By9/dirWt+f89c65qnul5pXXM2eaKirGtMbW0jdFLfxhr//wgmIwUANGkEEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcMJ+1jycPeFMGhjGBKaeyEj77UgyNTXWNbWlZfYb2mBf47XfyneqgxQ75WLrmp9O/JF1jVd/s66JaNPGukaSAhUV1jXhfF+EJZzJiiUpIozv90DjTJbKGRAAwAkCCADghFUA5ebm6uqrr1Z8fLw6dOigYcOGaevWrSFjKisrlZOTo3bt2ikuLk4jRoxQUVFRgzYNAGj+rAJo9erVysnJ0bp16/T++++rurpaQ4YMUcW3PiedMmWK3n77bb355ptavXq19u/fr+HDhzd44wCA5s3qJoTly5eHPM7Ly1OHDh20ceNG9e/fX2VlZfrd736nBQsWaODAgZKkefPm6eKLL9a6det07bXXNlznAIBm7TtdAyorO3Z3UmJioiRp48aNqq6u1uDBg4NjevTooc6dO2vt2rV1vkZVVZX8fn/IAgBo+cIOoEAgoMmTJ+v6669Xz549JUmFhYWKiYlR27ZtQ8YmJSWpsLCwztfJzc2Vz+cLLmlpaeG2BABoRsIOoJycHH3++edatGjRd2pg+vTpKisrCy579uz5Tq8HAGgewvpF1IkTJ+qdd97RmjVr1KlTp+D65ORkHT16VKWlpSFnQUVFRUpOTq7ztbxer7xefv0PAM41VmdAxhhNnDhRS5Ys0cqVK5Wenh7yfJ8+fRQdHa0VK1YE123dulW7d+9Wv379GqZjAECLYHUGlJOTowULFmjZsmWKj48PXtfx+Xxq1aqVfD6f7r//fk2dOlWJiYlKSEjQpEmT1K9fP+6AAwCEsAqgV155RZI0YMCAkPXz5s3TmDFjJEm/+tWvFBERoREjRqiqqkqZmZl6+eWXG6RZAEDL4TEmnBkvG4/f75fP59MADVWUJ9p1OwAASzWmWvlaprKyMiUkJNQ7jrngAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhhFUC5ubm6+uqrFR8frw4dOmjYsGHaunVryJgBAwbI4/GELA888ECDNg0AaP6sAmj16tXKycnRunXr9P7776u6ulpDhgxRRUVFyLhx48bpwIEDwWX27NkN2jQAoPmLshm8fPnykMd5eXnq0KGDNm7cqP79+wfXt27dWsnJyQ3TIQCgRfpO14DKysokSYmJiSHr58+fr/bt26tnz56aPn26Dh8+XO9rVFVVye/3hywAgJbP6gzo2wKBgCZPnqzrr79ePXv2DK6/55571KVLF6Wmpmrz5s165JFHtHXrVi1evLjO18nNzdWsWbPCbQMA0Ex5jDEmnMIJEybovffe04cffqhOnTrVO27lypUaNGiQtm/froyMjJOer6qqUlVVVfCx3+9XWlqaBmioojzR4bQGAHCoxlQrX8tUVlamhISEeseFdQY0ceJEvfPOO1qzZs0pw0eS+vbtK0n1BpDX65XX6w2nDQBAM2YVQMYYTZo0SUuWLFF+fr7S09NPW7Np0yZJUkpKSlgNAgBaJqsAysnJ0YIFC7Rs2TLFx8ersLBQkuTz+dSqVSvt2LFDCxYs0C233KJ27dpp8+bNmjJlivr376/LL7+8Ud4AAKB5sroG5PF46lw/b948jRkzRnv27NH3v/99ff7556qoqFBaWpruuOMOPf7446f8HPDb/H6/fD4f14AAoJlqlGtAp8uqtLQ0rV692uYlAQDnKOaCAwA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA4EeW6gRMZYyRJNaqWjONmAADWalQt6T//n9enyQVQeXm5JOlD/clxJwCA76K8vFw+n6/e5z3mdBF1lgUCAe3fv1/x8fHyeDwhz/n9fqWlpWnPnj1KSEhw1KF77Idj2A/HsB+OYT8c0xT2gzFG5eXlSk1NVURE/Vd6mtwZUEREhDp16nTKMQkJCef0AXYc++EY9sMx7Idj2A/HuN4PpzrzOY6bEAAAThBAAAAnmlUAeb1ezZw5U16v13UrTrEfjmE/HMN+OIb9cExz2g9N7iYEAMC5oVmdAQEAWg4CCADgBAEEAHCCAAIAOEEAAQCcaDYBNHfuXF1wwQWKjY1V37599cknn7hu6ax78skn5fF4QpYePXq4bqvRrVmzRrfddptSU1Pl8Xi0dOnSkOeNMZoxY4ZSUlLUqlUrDR48WNu2bXPTbCM63X4YM2bMScdHVlaWm2YbSW5urq6++mrFx8erQ4cOGjZsmLZu3RoyprKyUjk5OWrXrp3i4uI0YsQIFRUVOeq4cZzJfhgwYMBJx8MDDzzgqOO6NYsAeuONNzR16lTNnDlTn376qXr16qXMzEwdPHjQdWtn3aWXXqoDBw4Elw8//NB1S42uoqJCvXr10ty5c+t8fvbs2XrhhRf06quvav369WrTpo0yMzNVWVl5ljttXKfbD5KUlZUVcnwsXLjwLHbY+FavXq2cnBytW7dO77//vqqrqzVkyBBVVFQEx0yZMkVvv/223nzzTa1evVr79+/X8OHDHXbd8M5kP0jSuHHjQo6H2bNnO+q4HqYZuOaaa0xOTk7wcW1trUlNTTW5ubkOuzr7Zs6caXr16uW6DackmSVLlgQfBwIBk5ycbJ599tngutLSUuP1es3ChQsddHh2nLgfjDFm9OjRZujQoU76ceXgwYNGklm9erUx5tjXPjo62rz55pvBMf/617+MJLN27VpXbTa6E/eDMcbcdNNN5qGHHnLX1Blo8mdAR48e1caNGzV48ODguoiICA0ePFhr16512Jkb27ZtU2pqqrp27ap7771Xu3fvdt2SUwUFBSosLAw5Pnw+n/r27XtOHh/5+fnq0KGDunfvrgkTJqikpMR1S42qrKxMkpSYmChJ2rhxo6qrq0OOhx49eqhz584t+ng4cT8cN3/+fLVv3149e/bU9OnTdfjwYRft1avJzYZ9ouLiYtXW1iopKSlkfVJSkr744gtHXbnRt29f5eXlqXv37jpw4IBmzZqlG2+8UZ9//rni4+Ndt+dEYWGhJNV5fBx/7lyRlZWl4cOHKz09XTt27NBjjz2m7OxsrV27VpGRka7ba3CBQECTJ0/W9ddfr549e0o6djzExMSobdu2IWNb8vFQ136QpHvuuUddunRRamqqNm/erEceeURbt27V4sWLHXYbqskHEP4jOzs7+O/LL79cffv2VZcuXfQ///M/uv/++x12hqZg1KhRwX9fdtlluvzyy5WRkaH8/HwNGjTIYWeNIycnR59//vk5cR30VOrbD+PHjw/++7LLLlNKSooGDRqkHTt2KCMj42y3Wacm/xFc+/btFRkZedJdLEVFRUpOTnbUVdPQtm1bXXTRRdq+fbvrVpw5fgxwfJysa9euat++fYs8PiZOnKh33nlHq1atCvn7YcnJyTp69KhKS0tDxrfU46G+/VCXvn37SlKTOh6afADFxMSoT58+WrFiRXBdIBDQihUr1K9fP4eduXfo0CHt2LFDKSkprltxJj09XcnJySHHh9/v1/r168/542Pv3r0qKSlpUceHMUYTJ07UkiVLtHLlSqWnp4c836dPH0VHR4ccD1u3btXu3btb1PFwuv1Ql02bNklS0zoeXN8FcSYWLVpkvF6vycvLM//85z/N+PHjTdu2bU1hYaHr1s6qH//4xyY/P98UFBSYjz76yAwePNi0b9/eHDx40HVrjaq8vNx89tln5rPPPjOSzJw5c8xnn31mdu3aZYwx5r//+79N27ZtzbJly8zmzZvN0KFDTXp6ujly5IjjzhvWqfZDeXm5mTZtmlm7dq0pKCgwH3zwgbnyyitNt27dTGVlpevWG8yECROMz+cz+fn55sCBA8Hl8OHDwTEPPPCA6dy5s1m5cqXZsGGD6devn+nXr5/Drhve6fbD9u3bzVNPPWU2bNhgCgoKzLJly0zXrl1N//79HXceqlkEkDHGvPjii6Zz584mJibGXHPNNWbdunWuWzrr7rrrLpOSkmJiYmJMx44dzV133WW2b9/uuq1Gt2rVKiPppGX06NHGmGO3Yj/xxBMmKSnJeL1eM2jQILN161a3TTeCU+2Hw4cPmyFDhpjzzz/fREdHmy5duphx48a1uB/S6nr/ksy8efOCY44cOWIefPBBc95555nWrVubO+64wxw4cMBd043gdPth9+7dpn///iYxMdF4vV5z4YUXmocfftiUlZW5bfwE/D0gAIATTf4aEACgZSKAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACf+L07W3dOwlv87AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "\n",
        "train_fmnist_data = FashionMNIST(\n",
        "    \".\", train=True, transform=torchvision.transforms.ToTensor(), download=True\n",
        ")\n",
        "test_fmnist_data = FashionMNIST(\n",
        "    \".\", train=False, transform=torchvision.transforms.ToTensor(), download=True\n",
        ")\n",
        "\n",
        "\n",
        "train_data_loader = torch.utils.data.DataLoader(\n",
        "    train_fmnist_data, batch_size=32, shuffle=True, num_workers=2\n",
        ")\n",
        "\n",
        "test_data_loader = torch.utils.data.DataLoader(\n",
        "    test_fmnist_data, batch_size=32, shuffle=False, num_workers=2\n",
        ")\n",
        "\n",
        "random_batch = next(iter(train_data_loader))\n",
        "_image, _label = random_batch[0][0], random_batch[1][0]\n",
        "plt.figure()\n",
        "plt.imshow(_image.reshape(28, 28))\n",
        "plt.title(f\"Image label: {_label}\")\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6jWRv1rgSq8"
      },
      "source": [
        "Постройте модель ниже. Пожалуйста, не стройте переусложненную сеть, не стоит делать ее глубже четырех слоев (можно и меньше). Ваша основная задача – обучить модель и получить качество на отложенной (тестовой выборке) не менее 88.5% accuracy.\n",
        "\n",
        "__Внимание, ваша модель должна быть представлена именно переменной `model_task_1`. На вход ей должен приходить тензор размерностью (1, 28, 28).__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcyEFX-RgSq8"
      },
      "outputs": [],
      "source": [
        "# Creating model instance\n",
        "model_task_1 = torch.nn.Sequential(\n",
        "    torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.MaxPool2d(kernel_size=2),\n",
        "    torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.MaxPool2d(kernel_size=2),\n",
        "    torch.nn.Flatten(),\n",
        "    torch.nn.Linear(64 * 7 * 7, 128),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(128, 10)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAoLV4dkoy5M"
      },
      "source": [
        "Не забудьте перенести модель на выбранный `device`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xas9SIXDoxvZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e9d5611-d599-4ad8-90d2-111760d37a1e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (1): ReLU()\n",
              "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (4): ReLU()\n",
              "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (6): Flatten(start_dim=1, end_dim=-1)\n",
              "  (7): Linear(in_features=3136, out_features=128, bias=True)\n",
              "  (8): ReLU()\n",
              "  (9): Linear(in_features=128, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "model_task_1.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pLRWysggSq9"
      },
      "source": [
        "Локальные тесты для проверки вашей модели доступны ниже:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qMQzo1ggSq9",
        "outputId": "94c6a384-4c09-4f3a-eb50-e0768cc02797"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Everything seems fine!\n"
          ]
        }
      ],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "assert model_task_1 is not None, \"Please, use `model_task_1` variable to store your model\"\n",
        "\n",
        "try:\n",
        "    x = random_batch[0].to(device)\n",
        "    y = random_batch[1].to(device)\n",
        "\n",
        "    # compute outputs given inputs, both are variables\n",
        "    y_predicted = model_task_1(x)\n",
        "except Exception as e:\n",
        "    print(\"Something is wrong with the model\")\n",
        "    raise e\n",
        "\n",
        "\n",
        "assert y_predicted.shape[-1] == 10, \"Model should predict 10 logits/probas\"\n",
        "\n",
        "print(\"Everything seems fine!\")\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suRmIPwIgSq9"
      },
      "source": [
        "Настройте параметры модели на обучающей выборке. Также рекомендуем поработать с `learning rate`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJnU14bdnZa_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "ca44e5f3-bea4-4467-ef7c-ab3029805106"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 0.9182999730110168\n",
            "Epoch: 1, Loss: 0.9358166456222534\n",
            "Epoch: 2, Loss: 0.9454500079154968\n",
            "Epoch: 3, Loss: 0.956083357334137\n",
            "Epoch: 4, Loss: 0.9580833315849304\n",
            "Epoch: 5, Loss: 0.9577500224113464\n",
            "Epoch: 6, Loss: 0.9730666875839233\n",
            "Epoch: 7, Loss: 0.9764000177383423\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.AdamW(model_task_1.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(8):\n",
        "    model_task_1.train()\n",
        "    loss_function = torch.nn.CrossEntropyLoss()\n",
        "    for batch in train_data_loader:\n",
        "        x, y = batch[0].to(device), batch[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        y_predicted = model_task_1(x)\n",
        "        loss = loss_function(y_predicted, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
        "    print(f'Epoch: {epoch}, Loss: {get_accuracy(model_task_1, train_data_loader)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zce7gt1gSq-"
      },
      "source": [
        "Также, напоминаем, что в любой момент можно обратиться к замечательной [документации](https://pytorch.org/docs/stable/index.html) и [обучающим примерам](https://pytorch.org/tutorials/).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usswrWYOgSq-"
      },
      "source": [
        "Оценим качество классификации:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xua3TVZHgSq-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03259cc4-fa6f-44b7-8d46-7ce3604038fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural network accuracy on train set: 0.9764\n"
          ]
        }
      ],
      "source": [
        "train_acc_task_1 = get_accuracy(model_task_1, train_data_loader)\n",
        "print(f\"Neural network accuracy on train set: {train_acc_task_1:3.5}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9KEKXBxgSq-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8367c1a1-4aea-4ca0-8f23-d527531c0972"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural network accuracy on test set: 0.9187\n"
          ]
        }
      ],
      "source": [
        "test_acc_task_1 = get_accuracy(model_task_1, test_data_loader)\n",
        "print(f\"Neural network accuracy on test set: {test_acc_task_1:3.5}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oyhmMobgSq_"
      },
      "source": [
        "Проверка, что необходимые пороги пройдены:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAIrURCEgSq_"
      },
      "outputs": [],
      "source": [
        "assert test_acc_task_1 >= 0.885, \"Train accuracy is below 0.885 threshold\"\n",
        "assert (\n",
        "    train_acc_task_1 >= 0.905\n",
        "), \"Test accuracy is below 0.905 while test accuracy is fine. We recommend to check your model and data flow\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpLcsSYisRWj"
      },
      "source": [
        "Обращаем внимане, код ниже предполагает, что ваша модель имеет содержится в переменной `model_task_1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJ5Vty7xsRWj",
        "outputId": "34b7cf24-7e56-4aed-bcfc-bf02b24acabc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File saved to `submission_dict_task_1.json`\n"
          ]
        }
      ],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "assert os.path.exists(\n",
        "    \"hw_overfitting_data_dict.npy\"\n",
        "), \"Please, download `hw_overfitting_data_dict.npy` and place it in the working directory\"\n",
        "\n",
        "loaded_data_dict = np.load(\"hw_overfitting_data_dict.npy\", allow_pickle=True)\n",
        "\n",
        "submission_dict = {\n",
        "    \"train_predictions_task_1\": get_predictions(\n",
        "        model_task_1, torch.FloatTensor(loaded_data_dict.item()[\"train\"])\n",
        "    ),\n",
        "    \"test_predictions_task_1\": get_predictions(\n",
        "        model_task_1, torch.FloatTensor(loaded_data_dict.item()[\"test\"])\n",
        "    ),\n",
        "    \"model_task_1\": parse_pytorch_model(str(model_task_1)),\n",
        "}\n",
        "\n",
        "with open(\"submission_dict_task_1.json\", \"w\") as iofile:\n",
        "    json.dump(submission_dict, iofile)\n",
        "print(\"File saved to `submission_dict_task_1.json`\")\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjfaHLL6sRWj"
      },
      "source": [
        "### Задача №2: Переобучение (Initiation)\n",
        "Продолжим работу с набором данных [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist). Теперь ваша задача продемонстрировать переобучение модели на обучающей выборке. Достаточно показать, что точность классификации (не только функция потерь!) на тестовой выборке значительно отстает от обучающей.\n",
        "\n",
        "Обращаем ваше внимание, в задаче №3 вам придется починить данную модель (минимизировать эффект переобучения) с помощью механизмов регуляризации, поэтому не переусердствуйте!\n",
        "\n",
        "__Ваша вторая задача: реализовать используя пайплан обучения модели продемонстрировать переобучения модели на обучающей выборке.__\n",
        "\n",
        "Код для обучения модели вы можете переиспользовать. Далее присутствует лишь несколько тестов, которые помогут вам проверить свое решение."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZidGWPQIsRWj"
      },
      "source": [
        "Обращаем внимание, вам необходимо использовать переменную `model_task_2` для хранение модели во второй задаче.\n",
        "\n",
        "Не используйте `Dropout` и `BatchNorm` в этой задаче"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ye1jQdw8sRWj"
      },
      "outputs": [],
      "source": [
        "# Creating model instance\n",
        "# I want to use Small ResNet, with skip-connections\n",
        "# but without batchNorm\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.main_path = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=True)\n",
        "        )\n",
        "\n",
        "        # skip connection\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=True)\n",
        "            )\n",
        "    def forward(self, x):\n",
        "        return nn.functional.relu(self.main_path(x)+self.shortcut(x))\n",
        "\n",
        "class ResNet_overfit(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNet_overfit, self).__init__()\n",
        "\n",
        "        self.initial = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Res-blocks\n",
        "        self.res_block1 = ResidualBlock(64, 64)\n",
        "        self.res_block2 = ResidualBlock(64, 128, stride=2)\n",
        "        self.res_block3 = ResidualBlock(128, 256, stride=2)\n",
        "\n",
        "        # classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256, 10),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.initial(x)\n",
        "        x = self.res_block1(x)\n",
        "        x = self.res_block2(x)\n",
        "        x = self.res_block3(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "model_task_2 = ResNet_overfit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6T8FsRsNsRWj",
        "outputId": "bd4461c9-e81e-4ec9-b4de-492e13c693e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/20, Batch: 100, Loss: 0.3088\n",
            "Epoch: 1/20, Batch: 200, Loss: 0.2986\n",
            "Epoch: 1/20, Batch: 300, Loss: 0.2920\n",
            "Epoch: 1/20, Batch: 400, Loss: 0.2778\n",
            "Epoch: 1/20, Batch: 500, Loss: 0.2989\n",
            "Epoch: 1/20, Batch: 600, Loss: 0.2927\n",
            "Epoch: 1/20, Batch: 700, Loss: 0.2721\n",
            "Epoch: 1/20, Batch: 800, Loss: 0.2845\n",
            "Epoch: 1/20, Batch: 900, Loss: 0.2807\n",
            "Epoch: 1/20, Batch: 1000, Loss: 0.2811\n",
            "Epoch: 1/20, Batch: 1100, Loss: 0.3075\n",
            "Epoch: 1/20, Batch: 1200, Loss: 0.2818\n",
            "Epoch: 1/20, Batch: 1300, Loss: 0.2629\n",
            "Epoch: 1/20, Batch: 1400, Loss: 0.2710\n",
            "Epoch: 1/20, Batch: 1500, Loss: 0.2681\n",
            "Epoch: 1/20, Batch: 1600, Loss: 0.2782\n",
            "Epoch: 1/20, Batch: 1700, Loss: 0.2634\n",
            "Epoch: 1/20, Batch: 1800, Loss: 0.2700\n",
            "Epoch 1/20:\n",
            "  Train Accuracy: 0.9102\n",
            "  Test Accuracy: 0.8967\n",
            "  Gap: 0.0135\n",
            "Epoch: 2/20, Batch: 100, Loss: 0.2440\n",
            "Epoch: 2/20, Batch: 200, Loss: 0.2473\n",
            "Epoch: 2/20, Batch: 300, Loss: 0.2296\n",
            "Epoch: 2/20, Batch: 400, Loss: 0.2281\n",
            "Epoch: 2/20, Batch: 500, Loss: 0.2490\n",
            "Epoch: 2/20, Batch: 600, Loss: 0.2066\n",
            "Epoch: 2/20, Batch: 700, Loss: 0.2289\n",
            "Epoch: 2/20, Batch: 800, Loss: 0.2388\n",
            "Epoch: 2/20, Batch: 900, Loss: 0.2484\n",
            "Epoch: 2/20, Batch: 1000, Loss: 0.2215\n",
            "Epoch: 2/20, Batch: 1100, Loss: 0.2346\n",
            "Epoch: 2/20, Batch: 1200, Loss: 0.2558\n",
            "Epoch: 2/20, Batch: 1300, Loss: 0.2196\n",
            "Epoch: 2/20, Batch: 1400, Loss: 0.2070\n",
            "Epoch: 2/20, Batch: 1500, Loss: 0.2520\n",
            "Epoch: 2/20, Batch: 1600, Loss: 0.2196\n",
            "Epoch: 2/20, Batch: 1700, Loss: 0.2309\n",
            "Epoch: 2/20, Batch: 1800, Loss: 0.2433\n",
            "Epoch 2/20:\n",
            "  Train Accuracy: 0.9332\n",
            "  Test Accuracy: 0.9174\n",
            "  Gap: 0.0158\n",
            "Epoch: 3/20, Batch: 100, Loss: 0.1775\n",
            "Epoch: 3/20, Batch: 200, Loss: 0.1924\n",
            "Epoch: 3/20, Batch: 300, Loss: 0.2016\n",
            "Epoch: 3/20, Batch: 400, Loss: 0.2062\n",
            "Epoch: 3/20, Batch: 500, Loss: 0.2324\n",
            "Epoch: 3/20, Batch: 600, Loss: 0.1995\n",
            "Epoch: 3/20, Batch: 700, Loss: 0.1974\n",
            "Epoch: 3/20, Batch: 800, Loss: 0.2035\n",
            "Epoch: 3/20, Batch: 900, Loss: 0.2088\n",
            "Epoch: 3/20, Batch: 1000, Loss: 0.2134\n",
            "Epoch: 3/20, Batch: 1100, Loss: 0.2023\n",
            "Epoch: 3/20, Batch: 1200, Loss: 0.2195\n",
            "Epoch: 3/20, Batch: 1300, Loss: 0.2138\n",
            "Epoch: 3/20, Batch: 1400, Loss: 0.2053\n",
            "Epoch: 3/20, Batch: 1500, Loss: 0.1985\n",
            "Epoch: 3/20, Batch: 1600, Loss: 0.1924\n",
            "Epoch: 3/20, Batch: 1700, Loss: 0.2015\n",
            "Epoch: 3/20, Batch: 1800, Loss: 0.1849\n",
            "Epoch 3/20:\n",
            "  Train Accuracy: 0.9405\n",
            "  Test Accuracy: 0.9171\n",
            "  Gap: 0.0234\n",
            "Epoch: 4/20, Batch: 100, Loss: 0.1790\n",
            "Epoch: 4/20, Batch: 200, Loss: 0.1630\n",
            "Epoch: 4/20, Batch: 300, Loss: 0.1721\n",
            "Epoch: 4/20, Batch: 400, Loss: 0.1807\n",
            "Epoch: 4/20, Batch: 500, Loss: 0.1773\n",
            "Epoch: 4/20, Batch: 600, Loss: 0.1680\n",
            "Epoch: 4/20, Batch: 700, Loss: 0.1737\n",
            "Epoch: 4/20, Batch: 800, Loss: 0.1856\n",
            "Epoch: 4/20, Batch: 900, Loss: 0.1782\n",
            "Epoch: 4/20, Batch: 1000, Loss: 0.1864\n",
            "Epoch: 4/20, Batch: 1100, Loss: 0.1785\n",
            "Epoch: 4/20, Batch: 1200, Loss: 0.1936\n",
            "Epoch: 4/20, Batch: 1300, Loss: 0.1863\n",
            "Epoch: 4/20, Batch: 1400, Loss: 0.1760\n",
            "Epoch: 4/20, Batch: 1500, Loss: 0.1859\n",
            "Epoch: 4/20, Batch: 1600, Loss: 0.1779\n",
            "Epoch: 4/20, Batch: 1700, Loss: 0.1646\n",
            "Epoch: 4/20, Batch: 1800, Loss: 0.1816\n",
            "Epoch 4/20:\n",
            "  Train Accuracy: 0.9440\n",
            "  Test Accuracy: 0.9109\n",
            "  Gap: 0.0331\n",
            "Epoch: 5/20, Batch: 100, Loss: 0.1432\n",
            "Epoch: 5/20, Batch: 200, Loss: 0.1668\n",
            "Epoch: 5/20, Batch: 300, Loss: 0.1382\n",
            "Epoch: 5/20, Batch: 400, Loss: 0.1395\n",
            "Epoch: 5/20, Batch: 500, Loss: 0.1679\n",
            "Epoch: 5/20, Batch: 600, Loss: 0.1554\n",
            "Epoch: 5/20, Batch: 700, Loss: 0.1340\n",
            "Epoch: 5/20, Batch: 800, Loss: 0.1578\n",
            "Epoch: 5/20, Batch: 900, Loss: 0.1566\n",
            "Epoch: 5/20, Batch: 1000, Loss: 0.1522\n",
            "Epoch: 5/20, Batch: 1100, Loss: 0.1422\n",
            "Epoch: 5/20, Batch: 1200, Loss: 0.1655\n",
            "Epoch: 5/20, Batch: 1300, Loss: 0.1625\n",
            "Epoch: 5/20, Batch: 1400, Loss: 0.1571\n",
            "Epoch: 5/20, Batch: 1500, Loss: 0.1555\n",
            "Epoch: 5/20, Batch: 1600, Loss: 0.1554\n",
            "Epoch: 5/20, Batch: 1700, Loss: 0.1667\n",
            "Epoch: 5/20, Batch: 1800, Loss: 0.1690\n",
            "Epoch 5/20:\n",
            "  Train Accuracy: 0.9584\n",
            "  Test Accuracy: 0.9235\n",
            "  Gap: 0.0349\n",
            "Epoch: 6/20, Batch: 100, Loss: 0.1186\n",
            "Epoch: 6/20, Batch: 200, Loss: 0.1351\n",
            "Epoch: 6/20, Batch: 300, Loss: 0.1192\n",
            "Epoch: 6/20, Batch: 400, Loss: 0.1192\n",
            "Epoch: 6/20, Batch: 500, Loss: 0.1264\n",
            "Epoch: 6/20, Batch: 600, Loss: 0.1207\n",
            "Epoch: 6/20, Batch: 700, Loss: 0.1297\n",
            "Epoch: 6/20, Batch: 800, Loss: 0.1381\n",
            "Epoch: 6/20, Batch: 900, Loss: 0.1389\n",
            "Epoch: 6/20, Batch: 1000, Loss: 0.1263\n",
            "Epoch: 6/20, Batch: 1100, Loss: 0.1268\n",
            "Epoch: 6/20, Batch: 1200, Loss: 0.1311\n",
            "Epoch: 6/20, Batch: 1300, Loss: 0.1432\n",
            "Epoch: 6/20, Batch: 1400, Loss: 0.1437\n",
            "Epoch: 6/20, Batch: 1500, Loss: 0.1401\n",
            "Epoch: 6/20, Batch: 1600, Loss: 0.1125\n",
            "Epoch: 6/20, Batch: 1700, Loss: 0.1436\n",
            "Epoch: 6/20, Batch: 1800, Loss: 0.1420\n",
            "Epoch 6/20:\n",
            "  Train Accuracy: 0.9660\n",
            "  Test Accuracy: 0.9236\n",
            "  Gap: 0.0424\n",
            "Epoch: 7/20, Batch: 100, Loss: 0.0930\n",
            "Epoch: 7/20, Batch: 200, Loss: 0.1075\n",
            "Epoch: 7/20, Batch: 300, Loss: 0.0943\n",
            "Epoch: 7/20, Batch: 400, Loss: 0.1089\n",
            "Epoch: 7/20, Batch: 500, Loss: 0.1094\n",
            "Epoch: 7/20, Batch: 600, Loss: 0.0955\n",
            "Epoch: 7/20, Batch: 700, Loss: 0.1006\n",
            "Epoch: 7/20, Batch: 800, Loss: 0.1164\n",
            "Epoch: 7/20, Batch: 900, Loss: 0.1174\n",
            "Epoch: 7/20, Batch: 1000, Loss: 0.1174\n",
            "Epoch: 7/20, Batch: 1100, Loss: 0.1140\n",
            "Epoch: 7/20, Batch: 1200, Loss: 0.1042\n",
            "Epoch: 7/20, Batch: 1300, Loss: 0.1216\n",
            "Epoch: 7/20, Batch: 1400, Loss: 0.1142\n",
            "Epoch: 7/20, Batch: 1500, Loss: 0.1101\n",
            "Epoch: 7/20, Batch: 1600, Loss: 0.1094\n",
            "Epoch: 7/20, Batch: 1700, Loss: 0.1144\n",
            "Epoch: 7/20, Batch: 1800, Loss: 0.1068\n",
            "Epoch 7/20:\n",
            "  Train Accuracy: 0.9725\n",
            "  Test Accuracy: 0.9224\n",
            "  Gap: 0.0501\n",
            "Epoch: 8/20, Batch: 100, Loss: 0.0887\n",
            "Epoch: 8/20, Batch: 200, Loss: 0.0777\n",
            "Epoch: 8/20, Batch: 300, Loss: 0.0720\n",
            "Epoch: 8/20, Batch: 400, Loss: 0.0986\n",
            "Epoch: 8/20, Batch: 500, Loss: 0.0741\n",
            "Epoch: 8/20, Batch: 600, Loss: 0.0899\n",
            "Epoch: 8/20, Batch: 700, Loss: 0.0840\n",
            "Epoch: 8/20, Batch: 800, Loss: 0.0917\n",
            "Epoch: 8/20, Batch: 900, Loss: 0.1117\n",
            "Epoch: 8/20, Batch: 1000, Loss: 0.0897\n",
            "Epoch: 8/20, Batch: 1100, Loss: 0.0810\n",
            "Epoch: 8/20, Batch: 1200, Loss: 0.0865\n",
            "Epoch: 8/20, Batch: 1300, Loss: 0.1057\n",
            "Epoch: 8/20, Batch: 1400, Loss: 0.0728\n",
            "Epoch: 8/20, Batch: 1500, Loss: 0.0934\n",
            "Epoch: 8/20, Batch: 1600, Loss: 0.1041\n",
            "Epoch: 8/20, Batch: 1700, Loss: 0.1034\n",
            "Epoch: 8/20, Batch: 1800, Loss: 0.0910\n",
            "Epoch 8/20:\n",
            "  Train Accuracy: 0.9819\n",
            "  Test Accuracy: 0.9247\n",
            "  Gap: 0.0572\n",
            "Epoch: 9/20, Batch: 100, Loss: 0.0494\n",
            "Epoch: 9/20, Batch: 200, Loss: 0.0583\n",
            "Epoch: 9/20, Batch: 300, Loss: 0.0669\n",
            "Epoch: 9/20, Batch: 400, Loss: 0.0557\n",
            "Epoch: 9/20, Batch: 500, Loss: 0.0662\n",
            "Epoch: 9/20, Batch: 600, Loss: 0.0795\n",
            "Epoch: 9/20, Batch: 700, Loss: 0.0904\n",
            "Epoch: 9/20, Batch: 800, Loss: 0.0817\n",
            "Epoch: 9/20, Batch: 900, Loss: 0.0657\n",
            "Epoch: 9/20, Batch: 1000, Loss: 0.0983\n",
            "Epoch: 9/20, Batch: 1100, Loss: 0.0709\n",
            "Epoch: 9/20, Batch: 1200, Loss: 0.0727\n",
            "Epoch: 9/20, Batch: 1300, Loss: 0.0688\n",
            "Epoch: 9/20, Batch: 1400, Loss: 0.0962\n",
            "Epoch: 9/20, Batch: 1500, Loss: 0.0771\n",
            "Epoch: 9/20, Batch: 1600, Loss: 0.0758\n",
            "Epoch: 9/20, Batch: 1700, Loss: 0.0731\n",
            "Epoch: 9/20, Batch: 1800, Loss: 0.0824\n",
            "Epoch 9/20:\n",
            "  Train Accuracy: 0.9848\n",
            "  Test Accuracy: 0.9255\n",
            "  Gap: 0.0593\n",
            "Epoch: 10/20, Batch: 100, Loss: 0.0410\n",
            "Epoch: 10/20, Batch: 200, Loss: 0.0401\n",
            "Epoch: 10/20, Batch: 300, Loss: 0.0409\n",
            "Epoch: 10/20, Batch: 400, Loss: 0.0474\n",
            "Epoch: 10/20, Batch: 500, Loss: 0.0630\n",
            "Epoch: 10/20, Batch: 600, Loss: 0.0407\n",
            "Epoch: 10/20, Batch: 700, Loss: 0.0508\n",
            "Epoch: 10/20, Batch: 800, Loss: 0.0595\n",
            "Epoch: 10/20, Batch: 900, Loss: 0.0688\n",
            "Epoch: 10/20, Batch: 1000, Loss: 0.0613\n",
            "Epoch: 10/20, Batch: 1100, Loss: 0.0677\n",
            "Epoch: 10/20, Batch: 1200, Loss: 0.0506\n",
            "Epoch: 10/20, Batch: 1300, Loss: 0.0783\n",
            "Epoch: 10/20, Batch: 1400, Loss: 0.0748\n",
            "Epoch: 10/20, Batch: 1500, Loss: 0.0656\n",
            "Epoch: 10/20, Batch: 1600, Loss: 0.0649\n",
            "Epoch: 10/20, Batch: 1700, Loss: 0.0661\n",
            "Epoch: 10/20, Batch: 1800, Loss: 0.0619\n",
            "Epoch 10/20:\n",
            "  Train Accuracy: 0.9827\n",
            "  Test Accuracy: 0.9175\n",
            "  Gap: 0.0652\n",
            "Epoch: 11/20, Batch: 100, Loss: 0.0368\n",
            "Epoch: 11/20, Batch: 200, Loss: 0.0500\n",
            "Epoch: 11/20, Batch: 300, Loss: 0.0429\n",
            "Epoch: 11/20, Batch: 400, Loss: 0.0394\n",
            "Epoch: 11/20, Batch: 500, Loss: 0.0383\n",
            "Epoch: 11/20, Batch: 600, Loss: 0.0510\n",
            "Epoch: 11/20, Batch: 700, Loss: 0.0467\n",
            "Epoch: 11/20, Batch: 800, Loss: 0.0559\n",
            "Epoch: 11/20, Batch: 900, Loss: 0.0494\n",
            "Epoch: 11/20, Batch: 1000, Loss: 0.0589\n",
            "Epoch: 11/20, Batch: 1100, Loss: 0.0408\n",
            "Epoch: 11/20, Batch: 1200, Loss: 0.0660\n",
            "Epoch: 11/20, Batch: 1300, Loss: 0.0612\n",
            "Epoch: 11/20, Batch: 1400, Loss: 0.0445\n",
            "Epoch: 11/20, Batch: 1500, Loss: 0.0550\n",
            "Epoch: 11/20, Batch: 1600, Loss: 0.0579\n",
            "Epoch: 11/20, Batch: 1700, Loss: 0.0713\n",
            "Epoch: 11/20, Batch: 1800, Loss: 0.0511\n",
            "Epoch 11/20:\n",
            "  Train Accuracy: 0.9880\n",
            "  Test Accuracy: 0.9196\n",
            "  Gap: 0.0684\n",
            "Epoch: 12/20, Batch: 100, Loss: 0.0318\n",
            "Epoch: 12/20, Batch: 200, Loss: 0.0321\n",
            "Epoch: 12/20, Batch: 300, Loss: 0.0259\n",
            "Epoch: 12/20, Batch: 400, Loss: 0.0323\n",
            "Epoch: 12/20, Batch: 500, Loss: 0.0331\n",
            "Epoch: 12/20, Batch: 600, Loss: 0.0392\n",
            "Epoch: 12/20, Batch: 700, Loss: 0.0380\n",
            "Epoch: 12/20, Batch: 800, Loss: 0.0309\n",
            "Epoch: 12/20, Batch: 900, Loss: 0.0320\n",
            "Epoch: 12/20, Batch: 1000, Loss: 0.0522\n",
            "Epoch: 12/20, Batch: 1100, Loss: 0.0441\n",
            "Epoch: 12/20, Batch: 1200, Loss: 0.0560\n",
            "Epoch: 12/20, Batch: 1300, Loss: 0.0432\n",
            "Epoch: 12/20, Batch: 1400, Loss: 0.0423\n",
            "Epoch: 12/20, Batch: 1500, Loss: 0.0551\n",
            "Epoch: 12/20, Batch: 1600, Loss: 0.0566\n",
            "Epoch: 12/20, Batch: 1700, Loss: 0.0513\n",
            "Epoch: 12/20, Batch: 1800, Loss: 0.0498\n",
            "Epoch 12/20:\n",
            "  Train Accuracy: 0.9855\n",
            "  Test Accuracy: 0.9160\n",
            "  Gap: 0.0695\n",
            "Epoch: 13/20, Batch: 100, Loss: 0.0235\n",
            "Epoch: 13/20, Batch: 200, Loss: 0.0270\n",
            "Epoch: 13/20, Batch: 300, Loss: 0.0331\n",
            "Epoch: 13/20, Batch: 400, Loss: 0.0321\n",
            "Epoch: 13/20, Batch: 500, Loss: 0.0288\n",
            "Epoch: 13/20, Batch: 600, Loss: 0.0363\n",
            "Epoch: 13/20, Batch: 700, Loss: 0.0406\n",
            "Epoch: 13/20, Batch: 800, Loss: 0.0545\n",
            "Epoch: 13/20, Batch: 900, Loss: 0.0427\n",
            "Epoch: 13/20, Batch: 1000, Loss: 0.0412\n",
            "Epoch: 13/20, Batch: 1100, Loss: 0.0386\n",
            "Epoch: 13/20, Batch: 1200, Loss: 0.0296\n",
            "Epoch: 13/20, Batch: 1300, Loss: 0.0319\n",
            "Epoch: 13/20, Batch: 1400, Loss: 0.0446\n",
            "Epoch: 13/20, Batch: 1500, Loss: 0.0322\n",
            "Epoch: 13/20, Batch: 1600, Loss: 0.0328\n",
            "Epoch: 13/20, Batch: 1700, Loss: 0.0408\n",
            "Epoch: 13/20, Batch: 1800, Loss: 0.0501\n",
            "Epoch 13/20:\n",
            "  Train Accuracy: 0.9898\n",
            "  Test Accuracy: 0.9208\n",
            "  Gap: 0.0690\n",
            "Epoch: 14/20, Batch: 100, Loss: 0.0307\n",
            "Epoch: 14/20, Batch: 200, Loss: 0.0243\n",
            "Epoch: 14/20, Batch: 300, Loss: 0.0303\n",
            "Epoch: 14/20, Batch: 400, Loss: 0.0228\n",
            "Epoch: 14/20, Batch: 500, Loss: 0.0276\n",
            "Epoch: 14/20, Batch: 600, Loss: 0.0450\n",
            "Epoch: 14/20, Batch: 700, Loss: 0.0283\n",
            "Epoch: 14/20, Batch: 800, Loss: 0.0372\n",
            "Epoch: 14/20, Batch: 900, Loss: 0.0312\n",
            "Epoch: 14/20, Batch: 1000, Loss: 0.0462\n",
            "Epoch: 14/20, Batch: 1100, Loss: 0.0394\n",
            "Epoch: 14/20, Batch: 1200, Loss: 0.0407\n",
            "Epoch: 14/20, Batch: 1300, Loss: 0.0382\n",
            "Epoch: 14/20, Batch: 1400, Loss: 0.0466\n",
            "Epoch: 14/20, Batch: 1500, Loss: 0.0494\n",
            "Epoch: 14/20, Batch: 1600, Loss: 0.0367\n",
            "Epoch: 14/20, Batch: 1700, Loss: 0.0324\n",
            "Epoch: 14/20, Batch: 1800, Loss: 0.0336\n",
            "Epoch 14/20:\n",
            "  Train Accuracy: 0.9928\n",
            "  Test Accuracy: 0.9231\n",
            "  Gap: 0.0697\n",
            "\n",
            "Final Results:\n",
            "Neural network accuracy on train set: 0.9928\n",
            "Neural network accuracy on test set: 0.9231\n",
            "Accuracy gap (overfitting measure): 0.0697\n"
          ]
        }
      ],
      "source": [
        "# Move model to device\n",
        "model_task_2.to(device)\n",
        "\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(model_task_2.parameters(), lr=learning_rate, weight_decay=0.0)\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    model_task_2.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, (x, y) in enumerate(train_data_loader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        y_predicted = model_task_2(x)\n",
        "        loss = loss_function(y_predicted, y)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Print statistics\n",
        "        if batch_idx % 100 == 99:  # Print every 100 mini-batches\n",
        "            print(f'Epoch: {epoch+1}/{num_epochs}, Batch: {batch_idx+1}, Loss: {running_loss/100:.4f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "    # Check training and test accuracy after each epoch\n",
        "    train_acc = get_accuracy(model_task_2, train_data_loader)\n",
        "    test_acc = get_accuracy(model_task_2, test_data_loader)\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}:')\n",
        "    print(f'  Train Accuracy: {train_acc:.4f}')\n",
        "    print(f'  Test Accuracy: {test_acc:.4f}')\n",
        "    print(f'  Gap: {train_acc - test_acc:.4f}')\n",
        "\n",
        "    # Early stopping if we've achieved sufficient overfitting\n",
        "    if train_acc >= 0.99:\n",
        "        break\n",
        "\n",
        "# Final evaluation\n",
        "train_acc_task_2 = get_accuracy(model_task_2, train_data_loader)\n",
        "test_acc_task_2 = get_accuracy(model_task_2, test_data_loader)\n",
        "print(f\"\\nFinal Results:\")\n",
        "print(f\"Neural network accuracy on train set: {train_acc_task_2:.4f}\")\n",
        "print(f\"Neural network accuracy on test set: {test_acc_task_2:.4f}\")\n",
        "print(f\"Accuracy gap (overfitting measure): {train_acc_task_2 - test_acc_task_2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xmJt0kJsRWj"
      },
      "source": [
        "Проверка архитектуры:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQdZyTmGsRWj"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "layers_task_2 = []\n",
        "for element in parse_pytorch_model(str(model_task_2)).get(\"layers\", []):\n",
        "    layer_name = element[\"layer\"][\"type\"]\n",
        "    assert \"dropout\" not in layer_name.lower(), \"Do not use Dropout in Task 2!\"\n",
        "    assert \"batchnorm\" not in layer_name.lower(), \"Do not use BatchNorm in Task 2!\"\n",
        "    layers_task_2.append(layer_name)\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdnjP39XsRWj"
      },
      "source": [
        "Оценим качество классификации:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EljN1yeVsRWk",
        "outputId": "7558c9a8-bcf5-4455-a770-9f71157a5085"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural network accuracy on train set: 0.99282\n"
          ]
        }
      ],
      "source": [
        "train_acc_task_2 = get_accuracy(model_task_2, train_data_loader)\n",
        "print(f\"Neural network accuracy on train set: {train_acc_task_2:3.5}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLTUq7EZsRWk",
        "outputId": "8bf409bd-8aae-4b05-e949-1a61877edc81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural network accuracy on test set: 0.9231\n"
          ]
        }
      ],
      "source": [
        "test_acc_task_2 = get_accuracy(model_task_2, test_data_loader)\n",
        "print(f\"Neural network accuracy on test set: {test_acc_task_2:3.5}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fg8BrpXGsRWk"
      },
      "source": [
        "Проверка, что переобучение присутствует:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBSSM-q4sRWk"
      },
      "outputs": [],
      "source": [
        "assert train_acc_task_2 >= test_acc_task_2, \"Train accuracy must be higher than task accuracy\"\n",
        "assert train_acc_task_2 >= 0.88, \"Train accuracy must be higher than 0.88\"\n",
        "assert (\n",
        "    train_acc_task_2 - test_acc_task_2 >= 0.04\n",
        "), \"Test accuracy should be at least 0.04 lower that train.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHejUxqssRWk"
      },
      "source": [
        "Обращаем внимане, код ниже предполагает, что ваша модель имеет содержится в переменной `model_task_2`.\n",
        "\n",
        "Также предполагается, что в переменной `submission_dict` уже содержатся результаты задачи №1. Если их там нет, загрузите их из сохраненного файла в переменную перед запуском следующей ячейки."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjzY1prgsRWk",
        "outputId": "c20ac14d-a418-4250-c7a9-88b84b6ca8d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File saved to `submission_dict_tasks_1_and_2.json`\n"
          ]
        }
      ],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "assert os.path.exists(\n",
        "    \"hw_overfitting_data_dict.npy\"\n",
        "), \"Please, download `hw_overfitting_data_dict.npy` and place it in the working directory\"\n",
        "\n",
        "loaded_data_dict = np.load(\"hw_overfitting_data_dict.npy\", allow_pickle=True)\n",
        "\n",
        "submission_dict.update(\n",
        "    {\n",
        "        \"train_predictions_task_2\": get_predictions(\n",
        "            model_task_2, torch.FloatTensor(loaded_data_dict.item()[\"train\"])\n",
        "        ),\n",
        "        \"test_predictions_task_2\": get_predictions(\n",
        "            model_task_2, torch.FloatTensor(loaded_data_dict.item()[\"test\"])\n",
        "        ),\n",
        "        \"model_task_2\": parse_pytorch_model(str(model_task_2)),\n",
        "    }\n",
        ")\n",
        "\n",
        "with open(\"submission_dict_tasks_1_and_2.json\", \"w\") as iofile:\n",
        "    json.dump(submission_dict, iofile)\n",
        "print(\"File saved to `submission_dict_tasks_1_and_2.json`\")\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztuUDCvZsRWl"
      },
      "source": [
        "### Задача №3: Исправление модели (Return)\n",
        "Все так же работаем с [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist). Наконец, ваша задача исправить ~~ошибки прошлого~~ переобучение модели, построенной в задаче 2. Достаточно добиться расхождения между точностью классификации на обучающей и тестовой выборках не превышающего 0.015 (т.е. полутора процентов).\n",
        "\n",
        "Обращаем ваше внимание, архитектура модели в задаче №3 не должна существенно отличаться от задачи №2! Вы можете использовать Batchnorm, Dropout, уменьшить размерность промежуточных представлений, обратиться к аугментации данных, но вы не можете использовать меньшее количество слоёв.\n",
        "\n",
        "__Ваша третья и финальная задача: исправить модель и/или процесс обучения, дабы справиться с переобучением.__\n",
        "\n",
        "Код для обучения модели вы можете переиспользовать. Далее присутствует лишь несколько тестов, которые помогут вам проверить свое решение."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfFwuaBEsRWl"
      },
      "source": [
        "Обращаем внимание, вам необходимо использовать переменную `model_task_3` для хранение модели во второй задаче.\n",
        "\n",
        "Также код ниже будет обращаться к переменной `layers_task_2`, инициализируйте её, если она не определена."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMDafO24sRWl"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "assert (\n",
        "    layers_task_2 is not None\n",
        "), \"Initializa layers_task_2 vairable which contains list of layers in task 2 model\"\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKI5sY4esRWl",
        "outputId": "b95c247d-8742-471f-de28-4f2d10352f95"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet_stable(\n",
              "  (initial): Sequential(\n",
              "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "  )\n",
              "  (res_block1): ResidualBlock(\n",
              "    (main_path): Sequential(\n",
              "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (shortcut): Sequential()\n",
              "  )\n",
              "  (res_block2): ResidualBlock(\n",
              "    (main_path): Sequential(\n",
              "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (shortcut): Sequential(\n",
              "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (res_block3): ResidualBlock(\n",
              "    (main_path): Sequential(\n",
              "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (shortcut): Sequential(\n",
              "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (dropout1): Dropout(p=0.2, inplace=False)\n",
              "  (dropout2): Dropout(p=0.3, inplace=False)\n",
              "  (dropout3): Dropout(p=0.4, inplace=False)\n",
              "  (classifier): Sequential(\n",
              "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "    (1): Flatten(start_dim=1, end_dim=-1)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=256, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "# Improve previous model\n",
        "# add: BatchNorm, Dropout, weight decay\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.main_path = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels)\n",
        "        )\n",
        "\n",
        "        # skip connection\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return nn.functional.relu(self.main_path(x) + self.shortcut(x))\n",
        "\n",
        "class ResNet_stable(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNet_stable, self).__init__()\n",
        "\n",
        "        self.initial = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Res-blocks\n",
        "        self.res_block1 = ResidualBlock(64, 64)\n",
        "        self.res_block2 = ResidualBlock(64, 128, stride=2)\n",
        "        self.res_block3 = ResidualBlock(128, 256, stride=2)\n",
        "\n",
        "        # Dropout layers\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "        self.dropout3 = nn.Dropout(0.4)\n",
        "\n",
        "        # classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.initial(x)\n",
        "        x = self.res_block1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.res_block2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.res_block3(x)\n",
        "        x = self.dropout3(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "model_task_3 = ResNet_stable()\n",
        "model_task_3.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02oRDj6SsRWl",
        "outputId": "177bcd06-4984-4117-d8b9-5358edca1cfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/15, Batch: 100, Loss: 1.3291\n",
            "Epoch: 1/15, Batch: 200, Loss: 0.9149\n",
            "Epoch: 1/15, Batch: 300, Loss: 0.8282\n",
            "Epoch: 1/15, Batch: 400, Loss: 0.7242\n",
            "Epoch: 1/15, Batch: 500, Loss: 0.6604\n",
            "Epoch: 1/15, Batch: 600, Loss: 0.6365\n",
            "Epoch: 1/15, Batch: 700, Loss: 0.6091\n",
            "Epoch: 1/15, Batch: 800, Loss: 0.6385\n",
            "Epoch: 1/15, Batch: 900, Loss: 0.6074\n",
            "Epoch: 1/15, Batch: 1000, Loss: 0.5924\n",
            "Epoch: 1/15, Batch: 1100, Loss: 0.5801\n",
            "Epoch: 1/15, Batch: 1200, Loss: 0.5525\n",
            "Epoch: 1/15, Batch: 1300, Loss: 0.5174\n",
            "Epoch: 1/15, Batch: 1400, Loss: 0.5475\n",
            "Epoch: 1/15, Batch: 1500, Loss: 0.4954\n",
            "Epoch: 1/15, Batch: 1600, Loss: 0.5336\n",
            "Epoch: 1/15, Batch: 1700, Loss: 0.5199\n",
            "Epoch: 1/15, Batch: 1800, Loss: 0.5022\n",
            "Epoch 1/15:\n",
            "  Train Accuracy: 0.8230\n",
            "  Test Accuracy: 0.8168\n",
            "  Gap: 0.0062\n",
            "New best model saved with test accuracy: 0.8168\n",
            "Epoch: 2/15, Batch: 100, Loss: 0.4881\n",
            "Epoch: 2/15, Batch: 200, Loss: 0.4751\n",
            "Epoch: 2/15, Batch: 300, Loss: 0.4766\n",
            "Epoch: 2/15, Batch: 400, Loss: 0.4567\n",
            "Epoch: 2/15, Batch: 500, Loss: 0.4691\n",
            "Epoch: 2/15, Batch: 600, Loss: 0.4842\n",
            "Epoch: 2/15, Batch: 700, Loss: 0.4888\n",
            "Epoch: 2/15, Batch: 800, Loss: 0.4719\n",
            "Epoch: 2/15, Batch: 900, Loss: 0.4568\n",
            "Epoch: 2/15, Batch: 1000, Loss: 0.4478\n",
            "Epoch: 2/15, Batch: 1100, Loss: 0.4260\n",
            "Epoch: 2/15, Batch: 1200, Loss: 0.4465\n",
            "Epoch: 2/15, Batch: 1300, Loss: 0.4312\n",
            "Epoch: 2/15, Batch: 1400, Loss: 0.4579\n",
            "Epoch: 2/15, Batch: 1500, Loss: 0.4297\n",
            "Epoch: 2/15, Batch: 1600, Loss: 0.4404\n",
            "Epoch: 2/15, Batch: 1700, Loss: 0.4274\n",
            "Epoch: 2/15, Batch: 1800, Loss: 0.4316\n",
            "Epoch 2/15:\n",
            "  Train Accuracy: 0.8335\n",
            "  Test Accuracy: 0.8258\n",
            "  Gap: 0.0077\n",
            "New best model saved with test accuracy: 0.8258\n",
            "Epoch: 3/15, Batch: 100, Loss: 0.4431\n",
            "Epoch: 3/15, Batch: 200, Loss: 0.4452\n",
            "Epoch: 3/15, Batch: 300, Loss: 0.3908\n",
            "Epoch: 3/15, Batch: 400, Loss: 0.4156\n",
            "Epoch: 3/15, Batch: 500, Loss: 0.3841\n",
            "Epoch: 3/15, Batch: 600, Loss: 0.3928\n",
            "Epoch: 3/15, Batch: 700, Loss: 0.4042\n",
            "Epoch: 3/15, Batch: 800, Loss: 0.4140\n",
            "Epoch: 3/15, Batch: 900, Loss: 0.4135\n",
            "Epoch: 3/15, Batch: 1000, Loss: 0.3951\n",
            "Epoch: 3/15, Batch: 1100, Loss: 0.3832\n",
            "Epoch: 3/15, Batch: 1200, Loss: 0.4213\n",
            "Epoch: 3/15, Batch: 1300, Loss: 0.3756\n",
            "Epoch: 3/15, Batch: 1400, Loss: 0.4059\n",
            "Epoch: 3/15, Batch: 1500, Loss: 0.3818\n",
            "Epoch: 3/15, Batch: 1600, Loss: 0.4115\n",
            "Epoch: 3/15, Batch: 1700, Loss: 0.3781\n",
            "Epoch: 3/15, Batch: 1800, Loss: 0.3959\n",
            "Epoch 3/15:\n",
            "  Train Accuracy: 0.8942\n",
            "  Test Accuracy: 0.8873\n",
            "  Gap: 0.0069\n",
            "New best model saved with test accuracy: 0.8873\n",
            "Epoch: 4/15, Batch: 100, Loss: 0.3534\n",
            "Epoch: 4/15, Batch: 200, Loss: 0.3624\n",
            "Epoch: 4/15, Batch: 300, Loss: 0.3826\n",
            "Epoch: 4/15, Batch: 400, Loss: 0.3857\n",
            "Epoch: 4/15, Batch: 500, Loss: 0.3637\n",
            "Epoch: 4/15, Batch: 600, Loss: 0.3685\n",
            "Epoch: 4/15, Batch: 700, Loss: 0.3573\n",
            "Epoch: 4/15, Batch: 800, Loss: 0.3517\n",
            "Epoch: 4/15, Batch: 900, Loss: 0.3708\n",
            "Epoch: 4/15, Batch: 1000, Loss: 0.3927\n",
            "Epoch: 4/15, Batch: 1100, Loss: 0.3776\n",
            "Epoch: 4/15, Batch: 1200, Loss: 0.3472\n",
            "Epoch: 4/15, Batch: 1300, Loss: 0.3833\n",
            "Epoch: 4/15, Batch: 1400, Loss: 0.3606\n",
            "Epoch: 4/15, Batch: 1500, Loss: 0.3576\n",
            "Epoch: 4/15, Batch: 1600, Loss: 0.3558\n",
            "Epoch: 4/15, Batch: 1700, Loss: 0.3768\n",
            "Epoch: 4/15, Batch: 1800, Loss: 0.3541\n",
            "Epoch 4/15:\n",
            "  Train Accuracy: 0.9022\n",
            "  Test Accuracy: 0.8942\n",
            "  Gap: 0.0080\n",
            "New best model saved with test accuracy: 0.8942\n",
            "Epoch: 5/15, Batch: 100, Loss: 0.3736\n",
            "Epoch: 5/15, Batch: 200, Loss: 0.3653\n",
            "Epoch: 5/15, Batch: 300, Loss: 0.3629\n",
            "Epoch: 5/15, Batch: 400, Loss: 0.3693\n",
            "Epoch: 5/15, Batch: 500, Loss: 0.3391\n",
            "Epoch: 5/15, Batch: 600, Loss: 0.3459\n",
            "Epoch: 5/15, Batch: 700, Loss: 0.3647\n",
            "Epoch: 5/15, Batch: 800, Loss: 0.3357\n",
            "Epoch: 5/15, Batch: 900, Loss: 0.3419\n",
            "Epoch: 5/15, Batch: 1000, Loss: 0.3429\n",
            "Epoch: 5/15, Batch: 1100, Loss: 0.3343\n",
            "Epoch: 5/15, Batch: 1200, Loss: 0.3481\n",
            "Epoch: 5/15, Batch: 1300, Loss: 0.3618\n",
            "Epoch: 5/15, Batch: 1400, Loss: 0.3212\n",
            "Epoch: 5/15, Batch: 1500, Loss: 0.3291\n",
            "Epoch: 5/15, Batch: 1600, Loss: 0.3544\n",
            "Epoch: 5/15, Batch: 1700, Loss: 0.3305\n",
            "Epoch: 5/15, Batch: 1800, Loss: 0.3497\n",
            "Epoch 5/15:\n",
            "  Train Accuracy: 0.9055\n",
            "  Test Accuracy: 0.8954\n",
            "  Gap: 0.0101\n",
            "New best model saved with test accuracy: 0.8954\n",
            "Epoch: 6/15, Batch: 100, Loss: 0.3338\n",
            "Epoch: 6/15, Batch: 200, Loss: 0.3206\n",
            "Epoch: 6/15, Batch: 300, Loss: 0.3390\n",
            "Epoch: 6/15, Batch: 400, Loss: 0.3467\n",
            "Epoch: 6/15, Batch: 500, Loss: 0.3371\n",
            "Epoch: 6/15, Batch: 600, Loss: 0.3424\n",
            "Epoch: 6/15, Batch: 700, Loss: 0.3359\n",
            "Epoch: 6/15, Batch: 800, Loss: 0.3260\n",
            "Epoch: 6/15, Batch: 900, Loss: 0.3335\n",
            "Epoch: 6/15, Batch: 1000, Loss: 0.3300\n",
            "Epoch: 6/15, Batch: 1100, Loss: 0.3317\n",
            "Epoch: 6/15, Batch: 1200, Loss: 0.3116\n",
            "Epoch: 6/15, Batch: 1300, Loss: 0.3319\n",
            "Epoch: 6/15, Batch: 1400, Loss: 0.3522\n",
            "Epoch: 6/15, Batch: 1500, Loss: 0.3649\n",
            "Epoch: 6/15, Batch: 1600, Loss: 0.3402\n",
            "Epoch: 6/15, Batch: 1700, Loss: 0.3341\n",
            "Epoch: 6/15, Batch: 1800, Loss: 0.3439\n",
            "Epoch 6/15:\n",
            "  Train Accuracy: 0.9011\n",
            "  Test Accuracy: 0.8902\n",
            "  Gap: 0.0109\n",
            "Epoch: 7/15, Batch: 100, Loss: 0.3143\n",
            "Epoch: 7/15, Batch: 200, Loss: 0.3155\n",
            "Epoch: 7/15, Batch: 300, Loss: 0.3340\n",
            "Epoch: 7/15, Batch: 400, Loss: 0.3280\n",
            "Epoch: 7/15, Batch: 500, Loss: 0.3313\n",
            "Epoch: 7/15, Batch: 600, Loss: 0.3205\n",
            "Epoch: 7/15, Batch: 700, Loss: 0.3440\n",
            "Epoch: 7/15, Batch: 800, Loss: 0.3336\n",
            "Epoch: 7/15, Batch: 900, Loss: 0.3087\n",
            "Epoch: 7/15, Batch: 1000, Loss: 0.3211\n",
            "Epoch: 7/15, Batch: 1100, Loss: 0.3202\n",
            "Epoch: 7/15, Batch: 1200, Loss: 0.3189\n",
            "Epoch: 7/15, Batch: 1300, Loss: 0.3106\n",
            "Epoch: 7/15, Batch: 1400, Loss: 0.3292\n",
            "Epoch: 7/15, Batch: 1500, Loss: 0.2975\n",
            "Epoch: 7/15, Batch: 1600, Loss: 0.3351\n",
            "Epoch: 7/15, Batch: 1700, Loss: 0.3232\n",
            "Epoch: 7/15, Batch: 1800, Loss: 0.3268\n",
            "Epoch 7/15:\n",
            "  Train Accuracy: 0.9134\n",
            "  Test Accuracy: 0.9005\n",
            "  Gap: 0.0129\n",
            "New best model saved with test accuracy: 0.9005\n",
            "Epoch: 8/15, Batch: 100, Loss: 0.2881\n",
            "Epoch: 8/15, Batch: 200, Loss: 0.3002\n",
            "Epoch: 8/15, Batch: 300, Loss: 0.3042\n",
            "Epoch: 8/15, Batch: 400, Loss: 0.3102\n",
            "Epoch: 8/15, Batch: 500, Loss: 0.3074\n",
            "Epoch: 8/15, Batch: 600, Loss: 0.3330\n",
            "Epoch: 8/15, Batch: 700, Loss: 0.3196\n",
            "Epoch: 8/15, Batch: 800, Loss: 0.3169\n",
            "Epoch: 8/15, Batch: 900, Loss: 0.3052\n",
            "Epoch: 8/15, Batch: 1000, Loss: 0.3462\n",
            "Epoch: 8/15, Batch: 1100, Loss: 0.3106\n",
            "Epoch: 8/15, Batch: 1200, Loss: 0.3171\n",
            "Epoch: 8/15, Batch: 1300, Loss: 0.2960\n",
            "Epoch: 8/15, Batch: 1400, Loss: 0.2967\n",
            "Epoch: 8/15, Batch: 1500, Loss: 0.3096\n",
            "Epoch: 8/15, Batch: 1600, Loss: 0.3244\n",
            "Epoch: 8/15, Batch: 1700, Loss: 0.3177\n",
            "Epoch: 8/15, Batch: 1800, Loss: 0.3011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x792aa017ede0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       Exception ignored in: \n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x792aa017ede0>Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "    ^if w.is_alive():^^\n",
            " ^ ^ ^  ^ ^ ^^^^^^^^\n",
            "^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "    ^^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n",
            "^^^ ^ \n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "               ^^ ^    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError^^: can only test a child process^^\n",
            "^^^^^\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/15:\n",
            "  Train Accuracy: 0.9149\n",
            "  Test Accuracy: 0.9050\n",
            "  Gap: 0.0099\n",
            "New best model saved with test accuracy: 0.9050\n",
            "Epoch: 9/15, Batch: 100, Loss: 0.3107\n",
            "Epoch: 9/15, Batch: 200, Loss: 0.3250\n",
            "Epoch: 9/15, Batch: 300, Loss: 0.3159\n",
            "Epoch: 9/15, Batch: 400, Loss: 0.3109\n",
            "Epoch: 9/15, Batch: 500, Loss: 0.3054\n",
            "Epoch: 9/15, Batch: 600, Loss: 0.3193\n",
            "Epoch: 9/15, Batch: 700, Loss: 0.3028\n",
            "Epoch: 9/15, Batch: 800, Loss: 0.2912\n",
            "Epoch: 9/15, Batch: 900, Loss: 0.3055\n",
            "Epoch: 9/15, Batch: 1000, Loss: 0.3220\n",
            "Epoch: 9/15, Batch: 1100, Loss: 0.3096\n",
            "Epoch: 9/15, Batch: 1200, Loss: 0.3081\n",
            "Epoch: 9/15, Batch: 1300, Loss: 0.2927\n",
            "Epoch: 9/15, Batch: 1400, Loss: 0.3005\n",
            "Epoch: 9/15, Batch: 1500, Loss: 0.3229\n",
            "Epoch: 9/15, Batch: 1600, Loss: 0.2973\n",
            "Epoch: 9/15, Batch: 1700, Loss: 0.2906\n",
            "Epoch: 9/15, Batch: 1800, Loss: 0.3179\n",
            "Epoch 9/15:\n",
            "  Train Accuracy: 0.9165\n",
            "  Test Accuracy: 0.9050\n",
            "  Gap: 0.0115\n",
            "Epoch: 10/15, Batch: 100, Loss: 0.2889\n",
            "Epoch: 10/15, Batch: 200, Loss: 0.2984\n",
            "Epoch: 10/15, Batch: 300, Loss: 0.2865\n",
            "Epoch: 10/15, Batch: 400, Loss: 0.2936\n",
            "Epoch: 10/15, Batch: 500, Loss: 0.3005\n",
            "Epoch: 10/15, Batch: 600, Loss: 0.3218\n",
            "Epoch: 10/15, Batch: 700, Loss: 0.3161\n",
            "Epoch: 10/15, Batch: 800, Loss: 0.3070\n",
            "Epoch: 10/15, Batch: 900, Loss: 0.3030\n",
            "Epoch: 10/15, Batch: 1000, Loss: 0.2910\n",
            "Epoch: 10/15, Batch: 1100, Loss: 0.3057\n",
            "Epoch: 10/15, Batch: 1200, Loss: 0.2848\n",
            "Epoch: 10/15, Batch: 1300, Loss: 0.2980\n",
            "Epoch: 10/15, Batch: 1400, Loss: 0.3130\n",
            "Epoch: 10/15, Batch: 1500, Loss: 0.3037\n",
            "Epoch: 10/15, Batch: 1600, Loss: 0.2875\n",
            "Epoch: 10/15, Batch: 1700, Loss: 0.2897\n",
            "Epoch: 10/15, Batch: 1800, Loss: 0.2958\n",
            "Epoch 10/15:\n",
            "  Train Accuracy: 0.9183\n",
            "  Test Accuracy: 0.9090\n",
            "  Gap: 0.0093\n",
            "New best model saved with test accuracy: 0.9090\n",
            "Epoch: 11/15, Batch: 100, Loss: 0.2807\n",
            "Epoch: 11/15, Batch: 200, Loss: 0.2921\n",
            "Epoch: 11/15, Batch: 300, Loss: 0.3088\n",
            "Epoch: 11/15, Batch: 400, Loss: 0.3035\n",
            "Epoch: 11/15, Batch: 500, Loss: 0.3086\n",
            "Epoch: 11/15, Batch: 600, Loss: 0.2800\n",
            "Epoch: 11/15, Batch: 700, Loss: 0.2743\n",
            "Epoch: 11/15, Batch: 800, Loss: 0.3021\n",
            "Epoch: 11/15, Batch: 900, Loss: 0.2849\n",
            "Epoch: 11/15, Batch: 1000, Loss: 0.2947\n",
            "Epoch: 11/15, Batch: 1100, Loss: 0.2949\n",
            "Epoch: 11/15, Batch: 1200, Loss: 0.2865\n",
            "Epoch: 11/15, Batch: 1300, Loss: 0.3104\n",
            "Epoch: 11/15, Batch: 1400, Loss: 0.3101\n",
            "Epoch: 11/15, Batch: 1500, Loss: 0.2919\n",
            "Epoch: 11/15, Batch: 1600, Loss: 0.2878\n",
            "Epoch: 11/15, Batch: 1700, Loss: 0.3078\n",
            "Epoch: 11/15, Batch: 1800, Loss: 0.3028\n",
            "Epoch 11/15:\n",
            "  Train Accuracy: 0.9265\n",
            "  Test Accuracy: 0.9176\n",
            "  Gap: 0.0089\n",
            "New best model saved with test accuracy: 0.9176\n",
            "Epoch: 12/15, Batch: 100, Loss: 0.2974\n",
            "Epoch: 12/15, Batch: 200, Loss: 0.2741\n",
            "Epoch: 12/15, Batch: 300, Loss: 0.2890\n",
            "Epoch: 12/15, Batch: 400, Loss: 0.3085\n",
            "Epoch: 12/15, Batch: 500, Loss: 0.2693\n",
            "Epoch: 12/15, Batch: 600, Loss: 0.2698\n",
            "Epoch: 12/15, Batch: 700, Loss: 0.2863\n",
            "Epoch: 12/15, Batch: 800, Loss: 0.2878\n",
            "Epoch: 12/15, Batch: 900, Loss: 0.2903\n",
            "Epoch: 12/15, Batch: 1000, Loss: 0.2879\n",
            "Epoch: 12/15, Batch: 1100, Loss: 0.3324\n",
            "Epoch: 12/15, Batch: 1200, Loss: 0.2845\n",
            "Epoch: 12/15, Batch: 1300, Loss: 0.2736\n",
            "Epoch: 12/15, Batch: 1400, Loss: 0.2674\n",
            "Epoch: 12/15, Batch: 1500, Loss: 0.3042\n",
            "Epoch: 12/15, Batch: 1600, Loss: 0.2844\n",
            "Epoch: 12/15, Batch: 1700, Loss: 0.2827\n",
            "Epoch: 12/15, Batch: 1800, Loss: 0.2947\n",
            "Epoch 12/15:\n",
            "  Train Accuracy: 0.9179\n",
            "  Test Accuracy: 0.9070\n",
            "  Gap: 0.0109\n",
            "Epoch: 13/15, Batch: 100, Loss: 0.2645\n",
            "Epoch: 13/15, Batch: 200, Loss: 0.2756\n",
            "Epoch: 13/15, Batch: 300, Loss: 0.2843\n",
            "Epoch: 13/15, Batch: 400, Loss: 0.2955\n",
            "Epoch: 13/15, Batch: 500, Loss: 0.3031\n",
            "Epoch: 13/15, Batch: 600, Loss: 0.2937\n",
            "Epoch: 13/15, Batch: 700, Loss: 0.2988\n",
            "Epoch: 13/15, Batch: 800, Loss: 0.2947\n",
            "Epoch: 13/15, Batch: 900, Loss: 0.2875\n",
            "Epoch: 13/15, Batch: 1000, Loss: 0.2682\n",
            "Epoch: 13/15, Batch: 1100, Loss: 0.2899\n",
            "Epoch: 13/15, Batch: 1200, Loss: 0.2864\n",
            "Epoch: 13/15, Batch: 1300, Loss: 0.2896\n",
            "Epoch: 13/15, Batch: 1400, Loss: 0.2896\n",
            "Epoch: 13/15, Batch: 1500, Loss: 0.2645\n",
            "Epoch: 13/15, Batch: 1600, Loss: 0.2866\n",
            "Epoch: 13/15, Batch: 1700, Loss: 0.2889\n",
            "Epoch: 13/15, Batch: 1800, Loss: 0.2760\n",
            "Epoch 13/15:\n",
            "  Train Accuracy: 0.9260\n",
            "  Test Accuracy: 0.9172\n",
            "  Gap: 0.0088\n",
            "Epoch: 14/15, Batch: 100, Loss: 0.2550\n",
            "Epoch: 14/15, Batch: 200, Loss: 0.2800\n",
            "Epoch: 14/15, Batch: 300, Loss: 0.2884\n",
            "Epoch: 14/15, Batch: 400, Loss: 0.2699\n",
            "Epoch: 14/15, Batch: 500, Loss: 0.2834\n",
            "Epoch: 14/15, Batch: 600, Loss: 0.2768\n",
            "Epoch: 14/15, Batch: 700, Loss: 0.2776\n",
            "Epoch: 14/15, Batch: 800, Loss: 0.2817\n",
            "Epoch: 14/15, Batch: 900, Loss: 0.2791\n",
            "Epoch: 14/15, Batch: 1000, Loss: 0.3025\n",
            "Epoch: 14/15, Batch: 1100, Loss: 0.2862\n",
            "Epoch: 14/15, Batch: 1200, Loss: 0.3094\n",
            "Epoch: 14/15, Batch: 1300, Loss: 0.2794\n",
            "Epoch: 14/15, Batch: 1400, Loss: 0.3000\n",
            "Epoch: 14/15, Batch: 1500, Loss: 0.2878\n",
            "Epoch: 14/15, Batch: 1600, Loss: 0.2835\n",
            "Epoch: 14/15, Batch: 1700, Loss: 0.2688\n",
            "Epoch: 14/15, Batch: 1800, Loss: 0.2890\n",
            "Epoch 14/15:\n",
            "  Train Accuracy: 0.9239\n",
            "  Test Accuracy: 0.9109\n",
            "  Gap: 0.0130\n",
            "Early stopping triggered after 14 epochs\n",
            "Loaded the best model\n",
            "\n",
            "Final Results:\n",
            "Neural network accuracy on train set: 0.9265\n",
            "Neural network accuracy on test set: 0.9176\n",
            "Accuracy gap: 0.0089\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(model_task_3.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "transform_train = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.RandomAffine(degrees=5, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "    torchvision.transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_fmnist_data_aug = FashionMNIST(\n",
        "    \".\", train=True, transform=transform_train, download=False\n",
        ")\n",
        "\n",
        "train_data_loader_aug = torch.utils.data.DataLoader(\n",
        "    train_fmnist_data_aug, batch_size=32, shuffle=True, num_workers=2\n",
        ")\n",
        "\n",
        "num_epochs = 15\n",
        "best_test_acc = 0\n",
        "patience = 3\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model_task_3.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, (x, y) in enumerate(train_data_loader_aug):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        y_predicted = model_task_3(x)\n",
        "        loss = loss_function(y_predicted, y)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Print statistics\n",
        "        if batch_idx % 100 == 99:\n",
        "            print(f'Epoch: {epoch+1}/{num_epochs}, Batch: {batch_idx+1}, Loss: {running_loss/100:.4f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "    # Check training and test accuracy after each epoch\n",
        "    train_acc = get_accuracy(model_task_3, train_data_loader)  # Evaluate on non-augmented data\n",
        "    test_acc = get_accuracy(model_task_3, test_data_loader)\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}:')\n",
        "    print(f'  Train Accuracy: {train_acc:.4f}')\n",
        "    print(f'  Test Accuracy: {test_acc:.4f}')\n",
        "    print(f'  Gap: {train_acc - test_acc:.4f}')\n",
        "\n",
        "    # Early stopping based on test accuracy and gap\n",
        "    if test_acc > best_test_acc and (train_acc - test_acc) <= 0.015:\n",
        "        best_test_acc = test_acc\n",
        "        patience_counter = 0\n",
        "        torch.save(model_task_3.state_dict(), 'best_model_task_3.pth')\n",
        "        print(f\"New best model saved with test accuracy: {test_acc:.4f}\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "# Load the best model\n",
        "if os.path.exists('best_model_task_3.pth'):\n",
        "    model_task_3.load_state_dict(torch.load('best_model_task_3.pth'))\n",
        "    print(\"Loaded the best model\")\n",
        "\n",
        "train_acc_task_3 = get_accuracy(model_task_3, train_data_loader)\n",
        "test_acc_task_3 = get_accuracy(model_task_3, test_data_loader)\n",
        "print(f\"\\nFinal Results:\")\n",
        "print(f\"Neural network accuracy on train set: {train_acc_task_3:.4f}\")\n",
        "print(f\"Neural network accuracy on test set: {test_acc_task_3:.4f}\")\n",
        "print(f\"Accuracy gap: {train_acc_task_3 - test_acc_task_3:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEiWeBTysRWl"
      },
      "source": [
        "Проверка архитектуры:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QDUCMeZsRWl"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "layers_task_3 = []\n",
        "for element in parse_pytorch_model(str(model_task_3)).get(\"layers\", []):\n",
        "    layer_name = element[\"layer\"][\"type\"]\n",
        "    layers_task_3.append(layer_name)\n",
        "\n",
        "\n",
        "idx = 0\n",
        "for model_3_layer in layers_task_3:\n",
        "    model_2_layer = layers_task_2[idx]\n",
        "    if \"dropout\" not in model_3_layer.lower() and \"batchnorm\" not in model_3_layer.lower():\n",
        "        assert (\n",
        "            model_3_layer == model_2_layer\n",
        "        ), \"Models in tasks 2 and 3 must share the architecture except for Dropout and BatchNorm!\"\n",
        "        idx += 1\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpC52ihhsRWl"
      },
      "source": [
        "Оценим качество классификации:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJNqi1resRWl",
        "outputId": "5f0d42f5-427d-4cfc-de45-bbe9796af6b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural network accuracy on train set: 0.92653\n"
          ]
        }
      ],
      "source": [
        "train_acc_task_3 = get_accuracy(model_task_3, train_data_loader)\n",
        "print(f\"Neural network accuracy on train set: {train_acc_task_3:3.5}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0phlIxhsRWl",
        "outputId": "7607888c-c226-4a1b-f114-6dad4b8867c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural network accuracy on test set: 0.9176\n"
          ]
        }
      ],
      "source": [
        "test_acc_task_3 = get_accuracy(model_task_3, test_data_loader)\n",
        "print(f\"Neural network accuracy on test set: {test_acc_task_3:3.5}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhJWH81FsRWl"
      },
      "source": [
        "Проверка, что переобучение присутствует:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_qWBKl6sRWl"
      },
      "outputs": [],
      "source": [
        "assert train_acc_task_3 >= 0.88, \"Train accuracy must be higher than 0.88\"\n",
        "assert train_acc_task_3 >= 0.865, \"Test accuracy must be higher than 0.865\"\n",
        "assert (\n",
        "    train_acc_task_3 - test_acc_task_3 <= 0.015\n",
        "), \"Test accuracy should not be lower that train more than by 0.015\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRV6QqOwsRWl"
      },
      "source": [
        "Обращаем внимане, код ниже предполагает, что ваша модель имеет содержится в переменной `model_task_3`.\n",
        "\n",
        "Также предполагается, что в переменной `submission_dict` уже содержатся результаты задач №1 и №2. Если их там нет, загрузите их из сохраненных файлов перед запуском следующей ячейки."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pdLAChysRWl",
        "outputId": "a886e5c8-2411-4614-f65e-84ebde9ec2e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File saved to `submission_dict_final.json`\n"
          ]
        }
      ],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "assert os.path.exists(\n",
        "    \"hw_overfitting_data_dict.npy\"\n",
        "), \"Please, download `hw_overfitting_data_dict.npy` and place it in the working directory\"\n",
        "\n",
        "loaded_data_dict = np.load(\"hw_overfitting_data_dict.npy\", allow_pickle=True)\n",
        "\n",
        "submission_dict.update(\n",
        "    {\n",
        "        \"train_predictions_task_3\": get_predictions(\n",
        "            model_task_3, torch.FloatTensor(loaded_data_dict.item()[\"train\"])\n",
        "        ),\n",
        "        \"test_predictions_task_3\": get_predictions(\n",
        "            model_task_3, torch.FloatTensor(loaded_data_dict.item()[\"test\"])\n",
        "        ),\n",
        "        \"model_task_3\": parse_pytorch_model(str(model_task_3)),\n",
        "    }\n",
        ")\n",
        "\n",
        "with open(\"submission_dict_final.json\", \"w\") as iofile:\n",
        "    json.dump(submission_dict, iofile)\n",
        "print(\"File saved to `submission_dict_final.json`\")\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xai8JL3tgSq_"
      },
      "source": [
        "### Сдача задания\n",
        "Сдайте сгенерированные файлы в соответствующие задачи в соревновании, а именно:\n",
        "* `submission_dict_tasks_1_and_2.json` в задачу Initiation\n",
        "* `submission_dict_final.json` в задачу Return.\n",
        "\n",
        "\n",
        "`submission_dict_task_1.json` сдавать не нужно, он уже был сдан ранее."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtWnYAN_gSrA"
      },
      "source": [
        "На этом задание завершено. Поздравляем!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}