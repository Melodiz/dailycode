{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNtLJlW4v5VF"
   },
   "source": [
    "## Классификация текстов с использованием предобученных языковых моделей.\n",
    "\n",
    "В данном задании вам предстоит обратиться к задаче классификации текстов и решить ее с использованием предобученной модели BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import clear_output\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "\n",
    "%matplotlib inline\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратимся к набору данных SST-2. Holdout часть данных (которая понадобится вам для посылки) доступна по ссылке ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-23 16:28:37--  https://raw.githubusercontent.com/girafe-ai/ml-course/refs/heads/24f_yandex_ml_trainings/homeworks/hw04_bert_and_co/texts_holdout.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 51581 (50K) [text/plain]\n",
      "Saving to: ‘texts_holdout.json.4’\n",
      "\n",
      "texts_holdout.json. 100%[===================>]  50,37K  --.-KB/s    in 0,05s   \n",
      "\n",
      "2024-11-23 16:28:38 (946 KB/s) - ‘texts_holdout.json.4’ saved [51581/51581]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "\n",
    "!wget https://raw.githubusercontent.com/girafe-ai/ml-course/refs/heads/24f_yandex_ml_trainings/homeworks/hw04_bert_and_co/texts_holdout.json\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "df = pd.read_csv(\n",
    "    \"https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv\",\n",
    "    delimiter=\"\\t\",\n",
    "    header=None,\n",
    ")\n",
    "texts_train = df[0].values[:5000] # array of strings containign training texts\n",
    "y_train = df[1].values[:5000] # array of labels (0 or 1) for training texts\n",
    "texts_test = df[0].values[5000:] # array of strings containign test texts\n",
    "y_test = df[1].values[5000:] # array of labels (0 or 1) for test texts\n",
    "with open(\"texts_holdout.json\") as iofile:\n",
    "    texts_holdout = json.load(iofile)\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Весь остальной код предстоит написать вам.\n",
    "\n",
    "Для успешной сдачи на максимальный балл необходимо добиться хотя бы __84.5% accuracy на тестовой части выборки__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da44e421e2fa4f0dbb8df0941e715ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/4: 100%|██████████| 157/157 [06:39<00:00,  2.55s/it]\n",
      "Evaluating: 100%|██████████| 60/60 [00:37<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4, Test Accuracy: 0.8766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4: 100%|██████████| 157/157 [06:16<00:00,  2.40s/it]\n",
      "Evaluating: 100%|██████████| 60/60 [00:37<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/4, Test Accuracy: 0.8833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4: 100%|██████████| 157/157 [05:41<00:00,  2.17s/it]\n",
      "Evaluating: 100%|██████████| 60/60 [00:36<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/4, Test Accuracy: 0.8760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4: 100%|██████████| 157/157 [05:40<00:00,  2.17s/it]\n",
      "Evaluating: 100%|██████████| 60/60 [00:36<00:00,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/4, Test Accuracy: 0.8875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load pre-trained DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenize and encode the texts\n",
    "def tokenize_data(texts):\n",
    "    # Convert numpy array to list if necessary\n",
    "    if isinstance(texts, np.ndarray):\n",
    "        texts = texts.tolist()\n",
    "    return tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "# Tokenize train, test, and holdout data\n",
    "train_encodings = tokenize_data(texts_train)\n",
    "test_encodings = tokenize_data(texts_test)\n",
    "holdout_encodings = tokenize_data(texts_holdout)\n",
    "\n",
    "# Convert labels to tensors\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load pre-trained DistilBERT model\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 4\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Evaluation on test set\n",
    "    model.eval()\n",
    "    test_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, attention_mask, _ = batch\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
    "            test_preds.extend(preds)\n",
    "    \n",
    "    test_accuracy = accuracy_score(y_test, [1 if p > 0.5 else 0 for p in test_preds])\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 313/313 [02:01<00:00,  2.58it/s]\n",
      "Predicting: 100%|██████████| 120/120 [00:40<00:00,  2.98it/s]\n",
      "Predicting: 100%|██████████| 32/32 [00:09<00:00,  3.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# Predict probabilities for train, test, and holdout sets\n",
    "def predict_proba(texts, encodings):\n",
    "    dataset = TensorDataset(encodings['input_ids'], encodings['attention_mask'])\n",
    "    dataloader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "    \n",
    "    model.eval()\n",
    "    proba_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Predicting\"):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, attention_mask = batch\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            proba = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
    "            proba_list.extend(proba)\n",
    "    \n",
    "    return [float(p) for p in proba_list]\n",
    "\n",
    "train_proba = predict_proba(texts_train, train_encodings)\n",
    "test_proba = predict_proba(texts_test, test_encodings)\n",
    "holdout_proba = predict_proba(texts_holdout, holdout_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/7: 100%|██████████| 157/157 [05:48<00:00,  2.22s/it]\n",
      "Evaluating: 100%|██████████| 60/60 [00:38<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/7, Test Accuracy: 0.8792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/7: 100%|██████████| 157/157 [12:04<00:00,  4.61s/it] \n",
      "Evaluating: 100%|██████████| 60/60 [00:37<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/7, Test Accuracy: 0.8693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/7: 100%|██████████| 157/157 [05:46<00:00,  2.21s/it]\n",
      "Evaluating: 100%|██████████| 60/60 [00:36<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/7, Test Accuracy: 0.8766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 313/313 [01:46<00:00,  2.93it/s]\n",
      "Predicting: 100%|██████████| 120/120 [00:38<00:00,  3.10it/s]\n",
      "Predicting: 100%|██████████| 32/32 [00:09<00:00,  3.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# Continue training for 3 more epochs\n",
    "additional_epochs = 3\n",
    "total_epochs = num_epochs + additional_epochs\n",
    "\n",
    "for epoch in range(num_epochs, total_epochs):\n",
    "    model.train()\n",
    "    # Shuffle the training data\n",
    "    train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{total_epochs}\"):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Evaluation on test set\n",
    "    model.eval()\n",
    "    test_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, attention_mask, _ = batch\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
    "            test_preds.extend(preds)\n",
    "    \n",
    "    test_accuracy = accuracy_score(y_test, [1 if p > 0.5 else 0 for p in test_preds])\n",
    "    print(f\"Epoch {epoch + 1}/{total_epochs}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Recalculate probabilities for train, test, and holdout sets\n",
    "train_proba = predict_proba(texts_train, train_encodings)\n",
    "test_proba = predict_proba(texts_test, test_encodings)\n",
    "holdout_proba = predict_proba(texts_holdout, holdout_encodings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сдача взадания в контест\n",
    "Сохраните в словарь `out_dict` вероятности принадлежности к первому (положительному) классу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict = {\n",
    "    'train': train_proba,\n",
    "    'test': test_proba,\n",
    "    'holdout': holdout_proba\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "print(len(out_dict['holdout']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Несколько `assert`'ов для проверки вашей посылки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(out_dict[\"train\"], list), \"Object must be a list of floats\"\n",
    "assert isinstance(out_dict[\"train\"][0], float), \"Object must be a list of floats\"\n",
    "assert (\n",
    "    len(out_dict[\"train\"]) == 5000\n",
    "), \"The predicted probas list length does not match the train set size\"\n",
    "\n",
    "assert isinstance(out_dict[\"test\"], list), \"Object must be a list of floats\"\n",
    "assert isinstance(out_dict[\"test\"][0], float), \"Object must be a list of floats\"\n",
    "assert (\n",
    "    len(out_dict[\"test\"]) == 1920\n",
    "), \"The predicted probas list length does not match the test set size\"\n",
    "\n",
    "assert isinstance(out_dict[\"holdout\"], list), \"Object must be a list of floats\"\n",
    "assert isinstance(out_dict[\"holdout\"][0], float), \"Object must be a list of floats\"\n",
    "assert(\n",
    "    len(out_dict[\"holdout\"]) == 500\n",
    "), \"The predicted probas list length does not match the holdout set size\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустите код ниже для генерации посылки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to `submission_dict_hw_text_classification_with_bert.json`\n"
     ]
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "FILENAME = \"submission_dict_hw_text_classification_with_bert.json\"\n",
    "\n",
    "with open(FILENAME, \"w\") as iofile:\n",
    "    json.dump(out_dict, iofile)\n",
    "print(f\"File saved to `{FILENAME}`\")\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом задание завершено. Поздравляем!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_hw01_texts.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
