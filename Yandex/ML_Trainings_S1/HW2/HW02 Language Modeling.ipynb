{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация поэзии с помощью нейронных сетей: шаг 1\n",
    "##### Автор: [Радослав Нейчев](https://www.linkedin.com/in/radoslav-neychev/), @neychev\n",
    "\n",
    "Ваша основная задача: научиться генерироват стихи с помощью простой рекуррентной нейронной сети (Vanilla RNN). В качестве корпуса текстов для обучения будет выступать роман в стихах \"Евгений Онегин\" Александра Сергеевича Пушкина."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "import string\n",
    "import os\n",
    "from random import sample\n",
    "\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('{} device is available'.format(device))\n",
    "# __________end of block__________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MPenWOy01Ooa",
    "outputId": "a92e8e33-e009-4bd4-ac12-3b1b5e1cd3f2"
   },
   "source": [
    "#### 1. Загрузка данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "!wget https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/onegin.txt\n",
    "    \n",
    "with open('onegin.txt', 'r') as iofile:\n",
    "    text = iofile.readlines()\n",
    "    \n",
    "text = \"\".join([x.replace('\\t\\t', '').lower() for x in text])\n",
    "# __________end of block__________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "XQYpmGfR_gJ8"
   },
   "source": [
    "#### 2. Построение словаря и предобработка текста\n",
    "В данном задании требуется построить языковую модель на уровне символов. Приведем весь текст к нижнему регистру и построим словарь из всех символов в доступном корпусе текстов. Также добавим токен `<sos>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "tokens = sorted(set(text.lower())) + ['<sos>']\n",
    "num_tokens = len(tokens)\n",
    "\n",
    "assert num_tokens == 84, \"Check the tokenization process\"\n",
    "\n",
    "token_to_idx = {x: idx for idx, x in enumerate(tokens)}\n",
    "idx_to_token = {idx: x for idx, x in enumerate(tokens)}\n",
    "\n",
    "assert len(tokens) == len(token_to_idx), \"Mapping should be unique\"\n",
    "\n",
    "print(\"Seems fine!\")\n",
    "\n",
    "\n",
    "text_encoded = [token_to_idx[x] for x in text]\n",
    "# __________end of block__________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ваша задача__: обучить классическую рекуррентную нейронную сеть (Vanilla RNN) предсказывать следующий символ на полученном корпусе текстов и сгенерировать последовательность длины 100 для фиксированной начальной фразы.\n",
    "\n",
    "Вы можете воспользоваться кодом с занятие №6 или же обратиться к следующим ссылкам:\n",
    "* Замечательная статья за авторством Andrej Karpathy об использовании RNN: [link](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "* Пример char-rnn от Andrej Karpathy: [github repo](https://github.com/karpathy/char-rnn)\n",
    "* Замечательный пример генерации поэзии Шекспира: [github repo](https://github.com/spro/practical-pytorch/blob/master/char-rnn-generation/char-rnn-generation.ipynb)\n",
    "\n",
    "Данное задание является достаточно творческим. Не страшно, если поначалу оно вызывает затруднения. Последняя ссылка в списке выше может быть особенно полезна в данном случае.\n",
    "\n",
    "Далее для вашего удобства реализована функция, которая генерирует случайный батч размера `batch_size` из строк длиной `seq_length`. Вы можете использовать его при обучении модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "batch_size = 256\n",
    "seq_length = 100\n",
    "start_column = np.zeros((batch_size, 1), dtype=int) + token_to_idx['<sos>']\n",
    "\n",
    "def generate_chunk():\n",
    "    global text_encoded, start_column, batch_size, seq_length\n",
    "\n",
    "    start_index = np.random.randint(0, len(text_encoded) - batch_size*seq_length - 1)\n",
    "    data = np.array(text_encoded[start_index:start_index + batch_size*seq_length]).reshape((batch_size, -1))\n",
    "    yield np.hstack((start_column, data))\n",
    "# __________end of block__________    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример батча:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(generate_chunk())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее вам предстоит написать код для обучения модели и генерации текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RNN model with Dropout\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, num_tokens, hidden_size, num_layers=1, dropout_prob=0.5):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size=num_tokens, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size, num_tokens)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.dropout(out)  # Apply dropout to the RNN output\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "dropout_prob = 0.5  # Set dropout probability\n",
    "model = CharRNN(num_tokens, hidden_size, num_layers, dropout_prob).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 8_192\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for batch in generate_chunk():\n",
    "        inputs = torch.tensor(batch[:, :-1], dtype=torch.int64).to(device)\n",
    "        targets = torch.tensor(batch[:, 1:], dtype=torch.int64).to(device)\n",
    "\n",
    "        # One-hot encode inputs\n",
    "        inputs_one_hot = F.one_hot(inputs, num_classes=num_tokens).float()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs, hidden = model(inputs_one_hot, hidden)\n",
    "        loss = criterion(outputs.view(-1, num_tokens), targets.view(-1))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве иллюстрации ниже доступен график значений функции потерь, построенный в ходе обучения авторской сети (сам код для ее обучения вам и предстоит написать)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Шаблон функции `generate_sample` также доступен ниже. Вы можете как дозаполнить его, так и написать свою собственную функцию с нуля. Не забывайте, что все примеры в обучающей выборке начинались с токена `<sos>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text\n",
    "def generate_sample(char_rnn, seed_phrase=None, max_length=200, temperature=1.0, device=device):\n",
    "    if seed_phrase is not None:\n",
    "        x_sequence = [token_to_idx['<sos>']] + [token_to_idx[token] for token in seed_phrase]\n",
    "    else: \n",
    "        x_sequence = [token_to_idx['<sos>']]\n",
    "\n",
    "    x_sequence = torch.tensor([x_sequence], dtype=torch.int64).to(device)\n",
    "    hidden = char_rnn.init_hidden(1)\n",
    "\n",
    "    generated_text = seed_phrase if seed_phrase else ''\n",
    "    for _ in range(max_length - len(generated_text)):\n",
    "        inputs_one_hot = F.one_hot(x_sequence, num_classes=num_tokens).float()\n",
    "        outputs, hidden = char_rnn(inputs_one_hot, hidden)\n",
    "        outputs = outputs[:, -1, :] / temperature\n",
    "        probabilities = F.softmax(outputs, dim=-1).cpu().data.numpy()\n",
    "        next_token = np.random.choice(num_tokens, p=probabilities.ravel())\n",
    "        generated_text += idx_to_token[next_token]\n",
    "        x_sequence = torch.cat((x_sequence, torch.tensor([[next_token]], dtype=torch.int64).to(device)), dim=1)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_sample(char_rnn, seed_phrase=None, max_length=200, temperature=1.0, device=device):\n",
    "#     '''\n",
    "#     The function generates text given a phrase of length at least SEQ_LENGTH.\n",
    "#     :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n",
    "#     :param max_length: maximum output length, including seed_phrase\n",
    "#     :param temperature: coefficient for sampling.  higher temperature produces more chaotic outputs,\n",
    "#                         smaller temperature converges to the single most likely output\n",
    "#     '''\n",
    "    \n",
    "#     if seed_phrase is not None:\n",
    "#         x_sequence = [token_to_idx['<sos>']] + [token_to_idx[token] for token in seed_phrase]\n",
    "#     else: \n",
    "#         x_sequence = [token_to_idx['<sos>']]\n",
    "\n",
    "#     x_sequence = torch.tensor([x_sequence], dtype=torch.int64).to(device)\n",
    "    \n",
    "#     #feed the seed phrase, if any\n",
    "            \n",
    "#     # your code here\n",
    "    \n",
    "#     return ''.join([tokens[ix] for ix in x_sequence.cpu().data.numpy()[0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример текста сгенерированного обученной моделью доступен ниже. Не страшно, что в тексте много несуществующих слов. Используемая модель очень проста: это простая классическая RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_sample(model, ' мой дядя самых честных правил', max_length=500, temperature=0.8))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сдача задания\n",
    "Сгенерируйте десять последовательностей длиной 500, используя строку ' мой дядя самых честных правил'. Температуру для генерации выберите самостоятельно на основании визуального качества генериуремого текста. Не забудьте удалить все технические токены в случае их наличия.\n",
    "\n",
    "Сгенерированную последовательность сохрание в переменную `generated_phrase` и сдайте сгенерированный ниже файл в контест."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_phrase = ' мой дядя самых честных правил'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_phrases = [\n",
    "    generate_sample(model, seed_phrase, max_length=500, temperature=0.8).replace('<sos>', '')\n",
    "    for _ in range(10)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "\n",
    "import json\n",
    "if 'generated_phrases' not in locals():\n",
    "    raise ValueError(\"Please, save generated phrases to `generated_phrases` variable\")\n",
    "\n",
    "for phrase in generated_phrases:\n",
    "\n",
    "    if not isinstance(phrase, str):\n",
    "        raise ValueError(\"The generated phrase should be a string\")\n",
    "\n",
    "    if len(phrase) != 500:\n",
    "        raise ValueError(\"The `generated_phrase` length should be equal to 500\")\n",
    "\n",
    "    assert all([x in set(tokens) for x in set(list(phrase))]), 'Unknown tokens detected, check your submission!'\n",
    "    \n",
    "\n",
    "submission_dict = {\n",
    "    'token_to_idx': token_to_idx,\n",
    "    'generated_phrases': generated_phrases\n",
    "}\n",
    "\n",
    "with open('submission_dict.json', 'w') as iofile:\n",
    "    json.dump(submission_dict, iofile)\n",
    "print('File saved to `submission_dict.json`')\n",
    "# __________end of block__________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом задание завершено. Поздравляем!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "NLP HW Lab01_Poetry_generation.v5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
